# Runs - Common Core 2017 

#### ICT17ZCJL01 
[**`Results`**](./results.md#ict17zcjl01), [**`Participants`**](./participants.md#ictnet), [**`Proceedings`**](./proceedings.md#ictnet-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ICT17ZCJL01.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ICT17ZCJL01), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ICT17ZCJL01.pdf) 

- :material-rename: **Name:** ICT17ZCJL01 
- :fontawesome-solid-user-group: **Participant:** ICTNET 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/16/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `8fdc4809978c33a122d4be65c90074a4` 
- :material-text: **Run description:** Title only Solr Retrieve Framework 

---
#### ICT17ZCJL02 
[**`Results`**](./results.md#ict17zcjl02), [**`Participants`**](./participants.md#ictnet), [**`Proceedings`**](./proceedings.md#ictnet-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ICT17ZCJL02.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ICT17ZCJL02), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ICT17ZCJL02.pdf) 

- :material-rename: **Name:** ICT17ZCJL02 
- :fontawesome-solid-user-group: **Participant:** ICTNET 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/16/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `5ff4a9d1a8982d195a89a68c57c17d7a` 
- :material-text: **Run description:** Title only Solr Retrieve Framework Add some words manually from the corresponding desc to the query word bags 

---
#### ICT17ZCJL03 
[**`Results`**](./results.md#ict17zcjl03), [**`Participants`**](./participants.md#ictnet), [**`Proceedings`**](./proceedings.md#ictnet-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ICT17ZCJL03.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ICT17ZCJL03), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ICT17ZCJL03.pdf) 

- :material-rename: **Name:** ICT17ZCJL03 
- :fontawesome-solid-user-group: **Participant:** ICTNET 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/16/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `60eeb9d21b0f96477937954ca50a69e1` 
- :material-text: **Run description:** query expansion using the google news corpus above the ICT17ZCJL01 

---
#### ICT17ZCJL05 
[**`Results`**](./results.md#ict17zcjl05), [**`Participants`**](./participants.md#ictnet), [**`Proceedings`**](./proceedings.md#ictnet-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ICT17ZCJL05.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ICT17ZCJL05), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ICT17ZCJL05.pdf) 

- :material-rename: **Name:** ICT17ZCJL05 
- :fontawesome-solid-user-group: **Participant:** ICTNET 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/25/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `f0ee68fc20fb4b2bad6bde71f36df789` 
- :material-text: **Run description:** word2vector use the corpus solr search engine query expansion automatically 

---
#### ICT17ZCJL06 
[**`Results`**](./results.md#ict17zcjl06), [**`Participants`**](./participants.md#ictnet), [**`Proceedings`**](./proceedings.md#ictnet-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ICT17ZCJL06.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ICT17ZCJL06), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ICT17ZCJL06.pdf) 

- :material-rename: **Name:** ICT17ZCJL06 
- :fontawesome-solid-user-group: **Participant:** ICTNET 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/28/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `4757e30cfb4c6350ecb6e5621b6377ff` 
- :material-text: **Run description:** use the describtion below the topic 
- :material-code-tags: **Code:** [https://github.com/Didiao1758/trecFinalProject](https://github.com/Didiao1758/trecFinalProject) 

---
#### ICT17ZCJL07 
[**`Results`**](./results.md#ict17zcjl07), [**`Participants`**](./participants.md#ictnet), [**`Proceedings`**](./proceedings.md#ictnet-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ICT17ZCJL07.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ICT17ZCJL07), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ICT17ZCJL07.pdf) 

- :material-rename: **Name:** ICT17ZCJL07 
- :fontawesome-solid-user-group: **Participant:** ICTNET 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/28/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `f1b248ea2252005e1f5cf4d5e317c6fa` 
- :material-text: **Run description:** trim the weight for the query term 
- :material-code-tags: **Code:** [https://github.com/Didiao1758/trecFinalProject](https://github.com/Didiao1758/trecFinalProject) 

---
#### IlpsUvABoir 
[**`Results`**](./results.md#ilpsuvaboir), [**`Participants`**](./participants.md#uvailps), [**`Proceedings`**](./proceedings.md#ilps-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.IlpsUvABoir.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.IlpsUvABoir), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/IlpsUvABoir.pdf) 

- :material-rename: **Name:** IlpsUvABoir 
- :fontawesome-solid-user-group: **Participant:** UvA.ILPS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/14/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `3c4b3fb20d1a1cd0093c28d5f35a8b25` 
- :material-text: **Run description:** This run is generated via retrieval models in Indri where the model itself and the parameters are optimized by using Bayesian Optimization. Externel resources includeIndri, pybo, and the existing judgements of TREC robust track topics. 

---
#### IlpsUvANvsm 
[**`Results`**](./results.md#ilpsuvanvsm), [**`Participants`**](./participants.md#uvailps), [**`Proceedings`**](./proceedings.md#ilps-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.IlpsUvANvsm.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.IlpsUvANvsm), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/IlpsUvANvsm.pdf) 

- :material-rename: **Name:** IlpsUvANvsm 
- :fontawesome-solid-user-group: **Participant:** UvA.ILPS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/14/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `5a62715bd638bb0a8c7ff21a26ccb708` 
- :material-text: **Run description:** Title field only. This run is generated by a latent vector space method, named NVSM, currently under review at a conference. For topics 312 and 348, the method did not return a ranking as the vocabulary of the method is limited to the top-60k most frequent terms. For those topics, the method falls back to a QLM with Dirichlet smoothing (mu = 1000), as otherwise the TREC submission system does not allow us to submit. The URL specified above will contain the source code of the method. 
- :material-code-tags: **Code:** [http://github.com/cvangysel/cuNVSM](http://github.com/cvangysel/cuNVSM) 

---
#### IlpsUvAQlmNvsm 
[**`Results`**](./results.md#ilpsuvaqlmnvsm), [**`Participants`**](./participants.md#uvailps), [**`Proceedings`**](./proceedings.md#ilps-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.IlpsUvAQlmNvsm.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.IlpsUvAQlmNvsm), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/IlpsUvAQlmNvsm.pdf) 

- :material-rename: **Name:** IlpsUvAQlmNvsm 
- :fontawesome-solid-user-group: **Participant:** UvA.ILPS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/14/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `34b4fe1e4c25cd7f7e08f9a849781656` 
- :material-text: **Run description:** Title field only. This run is an unsupervised combination (per-topic standardised scores) between a QLM with Dirichlet smoothing (mu = 1000) and a latent vector space method, named NVSM, currently under review at a conference. The URL specified above will contain the source code of the method. 
- :material-code-tags: **Code:** [http://github.com/cvangysel/cuNVSM](http://github.com/cvangysel/cuNVSM) 

---
#### ims_bm25_td 
[**`Results`**](./results.md#ims_bm25_td), [**`Participants`**](./participants.md#baseline), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_bm25_td.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_bm25_td), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_bm25_td.pdf) 

- :material-rename: **Name:** ims_bm25_td 
- :fontawesome-solid-user-group: **Participant:** BASELINE 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/16/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `6d94337befb9cd97b3218aafbfff8cf2` 
- :material-text: **Run description:** Lucene 6.6.0 using components with default configuration: - tokenization: lucene StandardTokenizer - stop list: Indri stop list - stemmer: lucene Krovetz stemmer - ir model: lucene BM25 Used topic fields: title and description 
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_cmbsum 
[**`Results`**](./results.md#ims_cmbsum), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_cmbsum.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_cmbsum), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_cmbsum.pdf) 

- :material-rename: **Name:** ims_cmbsum 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/17/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `140705d3b7aa578f25c1232aab47d98c` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is (unsupervised) combsum with min-max normalization.  The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description.  
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_dfrinl2_td 
[**`Results`**](./results.md#ims_dfrinl2_td), [**`Participants`**](./participants.md#baseline), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_dfrinl2_td.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_dfrinl2_td), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_dfrinl2_td.pdf) 

- :material-rename: **Name:** ims_dfrinl2_td 
- :fontawesome-solid-user-group: **Participant:** BASELINE 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/16/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `0d760ffdad860bc57da682be3608d260` 
- :material-text: **Run description:** Lucene 6.6.0 using components with default configuration: - tokenization: lucene StandardTokenizer - stop list: Indri stop list - stemmer: lucene Krovetz stemmer - ir model: lucene DFR using Inverse Document Frequency model with Laplace's law of succession after-effect and normalisation 2 Used topic fields: title and description 
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcmbsum_ap 
[**`Results`**](./results.md#ims_wcmbsum_ap), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcmbsum_ap.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcmbsum_ap), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcmbsum_ap.pdf) 

- :material-rename: **Name:** ims_wcmbsum_ap 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/17/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `be132b040346acdf98bbe02b23571304` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its AP on that topic. AP is computed by scoring the same set of 330 systems on the TREC 13, 2004, Robust track. The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description. 
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcs_ap_uf 
[**`Results`**](./results.md#ims_wcs_ap_uf), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcs_ap_uf.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcs_ap_uf), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcs_ap_uf.pdf) 

- :material-rename: **Name:** ims_wcs_ap_uf 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `f32f11216ce0329e43d5c7e4e7632826` 
- :material-text: **Run description:** This run adopts a two-level data fusion approach. The runs are created using Lucene 6.6.0 and components with default configuration. In the first data fusion level, we use the lucene multi-scorer, an implementation of CombSum without normalization, to merge 11 IR models:  bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene  In the first data fusion level, we create a mini Grid-of-Points (GoP) consisting of 24 runs originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter - IR model: lucene multi-scores as above In the second data fusion level, we use weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its AP on that topic. AP is computed by scoring the same set of 24 systems on the TREC 13, 2004, Robust track. In addition, we ensure that the unique documents are at the top of the result lists, ranked by their min-max normalized score weighted by the AP of the run returning them. Used topic fields: title and description.  
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcs_err 
[**`Results`**](./results.md#ims_wcs_err), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcs_err.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcs_err), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcs_err.pdf) 

- :material-rename: **Name:** ims_wcs_err 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/22/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `d6cf75c81a8451a51acc07229ea4e03b` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its ERR on that topic. ERR is computed by scoring the same set of 330 systems on the TREC 13, 2004, Robust track. The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description. 
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcs_ndcg 
[**`Results`**](./results.md#ims_wcs_ndcg), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcs_ndcg.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcs_ndcg), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcs_ndcg.pdf) 

- :material-rename: **Name:** ims_wcs_ndcg 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/20/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `9f81dc1fe31b6ef395d19f37a9e31024` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its nDCG on that topic. nDCG is computed by scoring the same set of 330 systems on the TREC 13, 2004, Robust track. The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description. 
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcs_p10 
[**`Results`**](./results.md#ims_wcs_p10), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcs_p10.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcs_p10), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcs_p10.pdf) 

- :material-rename: **Name:** ims_wcs_p10 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/20/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `bdde1cc93019274b0edea6252c97769b` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its P@10 on that topic. P@10 is computed by scoring the same set of 330 systems on the TREC 13, 2004, Robust track. The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description. 
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcs_rbp 
[**`Results`**](./results.md#ims_wcs_rbp), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcs_rbp.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcs_rbp), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcs_rbp.pdf) 

- :material-rename: **Name:** ims_wcs_rbp 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/21/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `f2fccc912ad8cca48e2673741d232457` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its RBP on that topic. RBP is computed by scoring the same set of 330 systems on the TREC 13, 2004, Robust track. The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description. 
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcs_recall 
[**`Results`**](./results.md#ims_wcs_recall), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcs_recall.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcs_recall), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcs_recall.pdf) 

- :material-rename: **Name:** ims_wcs_recall 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/22/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `38391420273cce4fc1c3a25bee954043` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its Recall on that topic. Recall is computed by scoring the same set of 330 systems on the TREC 13, 2004, Robust track. The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description.  
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcs_rprec 
[**`Results`**](./results.md#ims_wcs_rprec), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcs_rprec.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcs_rprec), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcs_rprec.pdf) 

- :material-rename: **Name:** ims_wcs_rprec 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/23/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `f90af3c08bf7725f5d601c5fce8d4c6c` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its R-prec on that topic. R-prec is computed by scoring the same set of 330 systems on the TREC 13, 2004, Robust track. The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description.  
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### ims_wcs_twist 
[**`Results`**](./results.md#ims_wcs_twist), [**`Participants`**](./participants.md#ims-core), [**`Proceedings`**](./proceedings.md#ims-trec-2017-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.ims_wcs_twist.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.ims_wcs_twist), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/ims_wcs_twist.pdf) 

- :material-rename: **Name:** ims_wcs_twist 
- :fontawesome-solid-user-group: **Participant:** ims-core 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/21/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `88cbef1e08b952cc6b7141c75f1dc7c0` 
- :material-text: **Run description:** This run merges a Grid-of-Points (GoP) consisting of 330 weak open source baselines. The baselines are created using Lucene 6.6.0 and components with default configuration. The merging algorithm is weighted combsum with min-max normalization. For each topic, each contributing run is weighted by its Twist on that topic. Twist is computed by scoring the same set of 330 systems on the TREC 13, 2004, Robust track. The systems constituting the GoP are originated by a factorial combination of the following components: - tokenization: StandardTokenizer - stop list: nostop, indri, lucene, smart, snowball, terrier - stemmer: nostem, krovetz, lovins, porter, 5grams - IR model: bm25, dfichi, dfiis, dfrinb2, dfrinexpb2, dfrinl2, iblgd, ibspl, lmd, lmjm, lucene Used topic fields: title and description. 
- :material-code-tags: **Code:** [https://bitbucket.org/frrncl/trec-core-2017](https://bitbucket.org/frrncl/trec-core-2017) 

---
#### mpiik10e105akDT 
[**`Results`**](./results.md#mpiik10e105akdt), [**`Participants`**](./participants.md#mpiid5), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.mpiik10e105akDT.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.mpiik10e105akDT), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/mpiik10e105akDT.pdf) 

- :material-rename: **Name:** mpiik10e105akDT 
- :fontawesome-solid-user-group: **Participant:** MPIID5 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/19/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `cc699865b1dd6c82c64da6acd0db53f2` 
- :material-text: **Run description:** BM25 + Neural IR model trained on Robust04 
- :material-code-tags: **Code:** [https://github.com/khui/trec-core-track-17](https://github.com/khui/trec-core-track-17) 

---
#### mpiik10e111akDT 
[**`Results`**](./results.md#mpiik10e111akdt), [**`Participants`**](./participants.md#mpiid5), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.mpiik10e111akDT.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.mpiik10e111akDT), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/mpiik10e111akDT.pdf) 

- :material-rename: **Name:** mpiik10e111akDT 
- :fontawesome-solid-user-group: **Participant:** MPIID5 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/19/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `e069e6170fc284ef8f9a9309b12a498a` 
- :material-text: **Run description:** BM25 + Neural IR model trained on Robust04  
- :material-code-tags: **Code:** [https://github.com/khui/trec-core-track-17](https://github.com/khui/trec-core-track-17) 

---
#### mpiik15e74akDT 
[**`Results`**](./results.md#mpiik15e74akdt), [**`Participants`**](./participants.md#mpiid5), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.mpiik15e74akDT.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.mpiik15e74akDT), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/mpiik15e74akDT.pdf) 

- :material-rename: **Name:** mpiik15e74akDT 
- :fontawesome-solid-user-group: **Participant:** MPIID5 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/19/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `605286505917463b3fc79679d510e9f8` 
- :material-text: **Run description:** BM25 + Neural IR model trained on Robust04 
- :material-code-tags: **Code:** [https://github.com/khui/trec-core-track-17](https://github.com/khui/trec-core-track-17) 

---
#### MRGrandrel 
[**`Results`**](./results.md#mrgrandrel), [**`Participants`**](./participants.md#mrg_uwaterloo), [**`Proceedings`**](./proceedings.md#mrg-uwaterloo-and-waterloocormack-participation-in-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.MRGrandrel.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.MRGrandrel), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/MRGrandrel.pdf) 

- :material-rename: **Name:** MRGrandrel 
- :fontawesome-solid-user-group: **Participant:** MRG_UWaterloo 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/7/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `099192cb38f2724bea93a847f5609959` 
- :material-text: **Run description:** Manual run using the core engine of the TREC Total Recall Track Baseline Model Implementation (BMI). Docs judged:  42,587 (170/topic) Docs judged relevant: 30,124 (70.7%; 120/topic) Total judging time:  64.1 hrs (15.4 min/topic; 5.4 sec/doc) Run consists of all judged-relevant documents, in random order, followed by remaining documents, ranked by final learned model. NOTE:  Although random order for submission of judged-relevant documents is perhaps not realistic, it affords us an unbiased statistical estimate of precision (and hence a lower bound on R) regardless of the pooling depth.  

---
#### MRGrankall 
[**`Results`**](./results.md#mrgrankall), [**`Participants`**](./participants.md#mrg_uwaterloo), [**`Proceedings`**](./proceedings.md#mrg-uwaterloo-and-waterloocormack-participation-in-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.MRGrankall.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.MRGrankall), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/MRGrankall.pdf) 

- :material-rename: **Name:** MRGrankall 
- :fontawesome-solid-user-group: **Participant:** MRG_UWaterloo 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/7/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `eefdf96d526de80972af86cb238e87e8` 
- :material-text: **Run description:** NOTE:  This run differs from MRGrandrel and MRGrankrel in that manually judge-relevant documents are not explicitly placed at the top of the ranking.  All documents are ranked by the final model. Manual run using the core engine of the TREC Total Recall Track Baseline Model Implementation (BMI). Docs judged:  42,587 (170/topic) Docs judged relevant: 30,124 (70.7%; 120/topic) Total judging time:  64.1 hrs (15.4 min/topic; 5.4 sec/doc) Run consists of the top 10,000 documents, ranked by final learned model.  

---
#### MRGrankrel 
[**`Results`**](./results.md#mrgrankrel), [**`Participants`**](./participants.md#mrg_uwaterloo), [**`Proceedings`**](./proceedings.md#mrg-uwaterloo-and-waterloocormack-participation-in-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.MRGrankrel.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.MRGrankrel), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/MRGrankrel.pdf) 

- :material-rename: **Name:** MRGrankrel 
- :fontawesome-solid-user-group: **Participant:** MRG_UWaterloo 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/7/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `a543e76c88bd41673803ab56822bb75e` 
- :material-text: **Run description:** NOTE:  This run differs from MRGrandrel only in the order of the judged-relevant docuements. Manual run using the core engine of the TREC Total Recall Track Baseline Model Implementation (BMI). Docs judged:  42,587 (170/topic) Docs judged relevant: 30,124 (70.7%; 120/topic) Total judging time:  64.1 hrs (15.4 min/topic; 5.4 sec/doc) Run consists of all judged-relevant documents, ranked by the final learned model, followed by remaining documents, ranked by final learned model. NOTE:  Although post-hoc ranking for submission of judged-relevant documents is perhaps not realistic, it affords us the best opportunity to contribute relevant documents to the pool. 

---
#### RMITFDMQEA1 
[**`Results`**](./results.md#rmitfdmqea1), [**`Participants`**](./participants.md#rmit), [**`Proceedings`**](./proceedings.md#rmit-at-the-2017-trec-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.RMITFDMQEA1.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.RMITFDMQEA1), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/RMITFDMQEA1.pdf) 

- :material-rename: **Name:** RMITFDMQEA1 
- :fontawesome-solid-user-group: **Participant:** RMIT 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `814b42ba1e47c7eb763011b95db50da5` 
- :material-text: **Run description:** Automatic run over titles using FDM and RM3 query expansion. 

---
#### RMITRBCUQVT5M1 
[**`Results`**](./results.md#rmitrbcuqvt5m1), [**`Participants`**](./participants.md#rmit), [**`Proceedings`**](./proceedings.md#rmit-at-the-2017-trec-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.RMITRBCUQVT5M1.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.RMITRBCUQVT5M1), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/RMITRBCUQVT5M1.pdf) 

- :material-rename: **Name:** RMITRBCUQVT5M1 
- :fontawesome-solid-user-group: **Participant:** RMIT 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `1fdeb1b496dd69e80a01f938dbd518be` 
- :material-text: **Run description:** This is a fused run using RBC based on several hundred user generate queries for the information need. The top 5 user query variations were fused to produce the final run. Topics were ran using SDM+RM3, and Okapi BM25. The "best" five were based on their performance using the old TREC collection. 

---
#### RMITUQVBestM2 
[**`Results`**](./results.md#rmituqvbestm2), [**`Participants`**](./participants.md#rmit), [**`Proceedings`**](./proceedings.md#rmit-at-the-2017-trec-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.RMITUQVBestM2.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.RMITUQVBestM2), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/RMITUQVBestM2.pdf) 

- :material-rename: **Name:** RMITUQVBestM2 
- :fontawesome-solid-user-group: **Participant:** RMIT 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `cbfa2b567c5fb42caabdd8c470f09b22` 
- :material-text: **Run description:** Many user query variations were gathered from a group of users based on the original topic description and narratives. Queries were then ran using FDM+RM3. The "best" variation for each topic was selected based on performance using the old TREC collection. 

---
#### sab17coreA 
[**`Results`**](./results.md#sab17corea), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sab17coreA.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sab17coreA), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sab17coreA.pdf) 

- :material-rename: **Name:** sab17coreA 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/19/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `526b9fd5e5ff56fec0bd5deee63e4a87` 
- :material-text: **Run description:** Standard SMART Lnu.ltu vector run, based on full topic 

---
#### sab17coreE1 
[**`Results`**](./results.md#sab17coree1), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sab17coreE1.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sab17coreE1), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sab17coreE1.pdf) 

- :material-rename: **Name:** sab17coreE1 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/19/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `30563768ce2c1227acee4cfae7303a21` 
- :material-text: **Run description:** Constructed a very expanded query using Rocchio feedback, expanding by all terms in relevant documents in collection v45nocr that occur at least 3 times in collection. Base indexing Lnu.ltu, Rocchio weights 0,16,16 (0 weight from original query, equal weights from rel and nonrel docs). Will be used as input in later optimized weights run. 

---
#### sab17coreO1 
[**`Results`**](./results.md#sab17coreo1), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sab17coreO1.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sab17coreO1), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sab17coreO1.pdf) 

- :material-rename: **Name:** sab17coreO1 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/19/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `4a69ff258596cf981ebcb6478722eda1` 
- :material-text: **Run description:** 250 term queries heavily optimized using terms from relevant docs of v45nocr. These used the terms of sab17coreE1 as a pool of possible terms to add, and chose terms to maximize performance on v45nocr.  This should be the equivalent of the 2005 robust run sab05ror1. 

---
#### sabchmergeav45 
[**`Results`**](./results.md#sabchmergeav45), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sabchmergeav45.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sabchmergeav45), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sabchmergeav45.pdf) 

- :material-rename: **Name:** sabchmergeav45 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/1/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `d11a02d068817883afdec380a8a4309e` 
- :material-text: **Run description:** Optimized queries initially based on judgements from a collection merging v45 with aquaint, varying the number of top terms based on how well the optimized query of that length performed on the aquaint collection. Top terms sorted from pure rocchio feedback with no weight given to original query (i.e., terms ordered by average weight in rel docs minus average weight in nonrel docs). These queries are then merged with a pure original ltu query (equal weights for both) 

---
#### sabchoiceaqv45 
[**`Results`**](./results.md#sabchoiceaqv45), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sabchoiceaqv45.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sabchoiceaqv45), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sabchoiceaqv45.pdf) 

- :material-rename: **Name:** sabchoiceaqv45 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/1/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `6487452f587534e22c9c79c8e2e5935e` 
- :material-text: **Run description:** Optimized queries initially based on judgements from a collection merging v45 with aquaint, varying the number of top terms based on how well the optimized query of that length performed on the aquaint collection. Top terms sorted from pure rocchio feedback with no weight given to original query (i.e., terms ordered by average weight in rel docs minus average weight in nonrel docs). If the base (no expansion/optimization) query performs better on aquaint than any optimization query, it is used instead. 

---
#### sabchoicev45 
[**`Results`**](./results.md#sabchoicev45), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sabchoicev45.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sabchoicev45), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sabchoicev45.pdf) 

- :material-rename: **Name:** sabchoicev45 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/1/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `1c1a64f2cadca751085877aa837d0d12` 
- :material-text: **Run description:** Optimized queries based on v45 judgements, varying the number of terms based on how well the optimized query of that length performed on the aquaint collection (for the 33 queries with judgements on aquaint - used 50 terms otherwise). If the base query (no expansion or optimization) performed better on aquaint, it was used instead. 

---
#### sabmerge50aqv45 
[**`Results`**](./results.md#sabmerge50aqv45), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sabmerge50aqv45.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sabmerge50aqv45), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sabmerge50aqv45.pdf) 

- :material-rename: **Name:** sabmerge50aqv45 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/1/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `832dbe9188d6ff1a10689cf141f942ca` 
- :material-text: **Run description:** Optimized queries initially based on judgements from a collection merging v45 with aquaint, all queries used top 50 terms from pure rocchio feedback with no weight given to original query (i.e., terms ordered by average weight in rel docs minus average weight in nonrel docs). These queries are then merged with a pure original ltu query (equal weights for both) 

---
#### sabopt50av45 
[**`Results`**](./results.md#sabopt50av45), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sabopt50av45.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sabopt50av45), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sabopt50av45.pdf) 

- :material-rename: **Name:** sabopt50av45 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/1/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `3993cbda9ad143217f437d7df045af60` 
- :material-text: **Run description:** Optimized queries based on judgements from a collection merging v45 with aquaint, all queries used top 50 terms from pure rocchio feedback with no weight given to original query (i.e., terms ordered by average weight in rel docs minus average weight in nonrel docs). 

---
#### sabopt50v45 
[**`Results`**](./results.md#sabopt50v45), [**`Participants`**](./participants.md#sabir), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.sabopt50v45.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.sabopt50v45), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/sabopt50v45.pdf) 

- :material-rename: **Name:** sabopt50v45 
- :fontawesome-solid-user-group: **Participant:** Sabir 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/1/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `23ea422184f0b859413ad402a9669863` 
- :material-text: **Run description:** Optimized queries based on v45 judgements, all queries used top 50 terms from pure rocchio feedback with no weight given to original query (i.e., terms ordered by average weight in rel docs minus average weight in nonrel docs). 

---
#### tgncorpBASE 
[**`Results`**](./results.md#tgncorpbase), [**`Participants`**](./participants.md#tgncorp), [**`Proceedings`**](./proceedings.md#tarragon-consulting-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.tgncorpBASE.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.tgncorpBASE), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/tgncorpBASE.pdf) 

- :material-rename: **Name:** tgncorpBASE 
- :fontawesome-solid-user-group: **Participant:** tgncorp 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/11/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `61f166117ba80e24fedf9c853d4408b5` 
- :material-text: **Run description:** Solr queries semi-automatically constructed from the Topic descriptions and then augmented with information from Wordnet and Wikipedia. 

---
#### tgncorpBOOST 
[**`Results`**](./results.md#tgncorpboost), [**`Participants`**](./participants.md#tgncorp), [**`Proceedings`**](./proceedings.md#tarragon-consulting-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.tgncorpBOOST.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.tgncorpBOOST), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/tgncorpBOOST.pdf) 

- :material-rename: **Name:** tgncorpBOOST 
- :fontawesome-solid-user-group: **Participant:** tgncorp 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/11/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `fe08fd2dc25bb16b7d8add3582c508bd` 
- :material-text: **Run description:** Solr queries semi-automatically constructed from the Topic descriptions and then augmented with information from Wordnet and Wikipedia. This run re-ranks based on presence of auxiliary evidence. 

---
#### udelIndri 
[**`Results`**](./results.md#udelindri), [**`Participants`**](./participants.md#udel), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.udelIndri.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.udelIndri), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/udelIndri.pdf) 

- :material-rename: **Name:** udelIndri 
- :fontawesome-solid-user-group: **Participant:** udel 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `e7fd0826c62e5485a20dad36ea6894bb` 
- :material-text: **Run description:** basic Indri run with default parameter settings 

---
#### udelIndriB 
[**`Results`**](./results.md#udelindrib), [**`Participants`**](./participants.md#udel), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.udelIndriB.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.udelIndriB), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/udelIndriB.pdf) 

- :material-rename: **Name:** udelIndriB 
- :fontawesome-solid-user-group: **Participant:** udel 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `abc2d0dbbb40ec393ae861f9b729bb96` 
- :material-text: **Run description:** basic Indri run with default parameter settings 

---
#### UDelInfoEXPint 
[**`Results`**](./results.md#udelinfoexpint), [**`Participants`**](./participants.md#udel_fang), [**`Proceedings`**](./proceedings.md#evaluating-axiomatic-retrieval-models-in-the-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UDelInfoEXPint.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UDelInfoEXPint), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UDelInfoEXPint.pdf) 

- :material-rename: **Name:** UDelInfoEXPint 
- :fontawesome-solid-user-group: **Participant:** udel_fang 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `ddccf2d17a35cab14d94100ffaebfd0f` 
- :material-text: **Run description:** The basic retrieval method is F2EXP. Axiomatic query expansion is applied with the original collection to select the expansion terms. 

---
#### UDelInfoLOGext 
[**`Results`**](./results.md#udelinfologext), [**`Participants`**](./participants.md#udel_fang), [**`Proceedings`**](./proceedings.md#evaluating-axiomatic-retrieval-models-in-the-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UDelInfoLOGext.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UDelInfoLOGext), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UDelInfoLOGext.pdf) 

- :material-rename: **Name:** UDelInfoLOGext 
- :fontawesome-solid-user-group: **Participant:** udel_fang 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `9da3e5d8379ee17b980901c3ba03de60` 
- :material-text: **Run description:** The basic retrieval method is F2LOG. Snippets from famous search engines are used as the sources for query expansion.  

---
#### UDelInfoLOGint 
[**`Results`**](./results.md#udelinfologint), [**`Participants`**](./participants.md#udel_fang), [**`Proceedings`**](./proceedings.md#evaluating-axiomatic-retrieval-models-in-the-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UDelInfoLOGint.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UDelInfoLOGint), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UDelInfoLOGint.pdf) 

- :material-rename: **Name:** UDelInfoLOGint 
- :fontawesome-solid-user-group: **Participant:** udel_fang 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `720d5ff077a98dd2e6381589ee98f2c3` 
- :material-text: **Run description:** The basic retrieval method is F2LOG. Axiomatic query expansion is applied with the original collection 

---
#### umass_baselnrm 
[**`Results`**](./results.md#umass_baselnrm), [**`Participants`**](./participants.md#baseline), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_baselnrm.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_baselnrm), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_baselnrm.pdf) 

- :material-rename: **Name:** umass_baselnrm 
- :fontawesome-solid-user-group: **Participant:** BASELINE 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/15/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `663f7118071c8414ebe857334df1787e` 
- :material-text: **Run description:** Baseline run using Galago #rm (Relevance model) operator configured for top 10 feedback documents and 10 expansion terms. 

---
#### umass_baselnsdm 
[**`Results`**](./results.md#umass_baselnsdm), [**`Participants`**](./participants.md#baseline), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_baselnsdm.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_baselnsdm), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_baselnsdm.pdf) 

- :material-rename: **Name:** umass_baselnsdm 
- :fontawesome-solid-user-group: **Participant:** BASELINE 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/15/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `04b0e6f06ac52e54d1dc6fca0fc0e2af` 
- :material-text: **Run description:** Baseline run using Galago #sdm (Sequential Dependence Model) operator on topic title terms. 

---
#### umass_direlm 
[**`Results`**](./results.md#umass_direlm), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_direlm.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_direlm), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_direlm.pdf) 

- :material-rename: **Name:** umass_direlm 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/15/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `de9227ddec2be3a0ece3b756cca823c3` 
- :material-text: **Run description:** LambdaMART ranking with validation set using DIRE re-ranking model trained on topic titles. 

---
#### umass_direlmnvs 
[**`Results`**](./results.md#umass_direlmnvs), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_direlmnvs.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_direlmnvs), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_direlmnvs.pdf) 

- :material-rename: **Name:** umass_direlmnvs 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/15/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `82fe89283dc8e6248f754b21f7ef16ea` 
- :material-text: **Run description:** LambdaMART ranking without validation set using DIRE re-ranking model trained on topic titles. 

---
#### umass_diremart 
[**`Results`**](./results.md#umass_diremart), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_diremart.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_diremart), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_diremart.pdf) 

- :material-rename: **Name:** umass_diremart 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/15/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `31950a78e06dba48a07b184114363946` 
- :material-text: **Run description:** MART ranking with validation set using DIRE re-ranking model trained on topic titles. 

---
#### umass_emb1 
[**`Results`**](./results.md#umass_emb1), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_emb1.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_emb1), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_emb1.pdf) 

- :material-rename: **Name:** umass_emb1 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `42ab9c5ee1d366a159a41cdc0d7da163` 
- :material-text: **Run description:** Query expansion using word embeddings learned by neural nets.  Assume conditional query term independence. 

---
#### umass_erm 
[**`Results`**](./results.md#umass_erm), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_erm.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_erm), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_erm.pdf) 

- :material-rename: **Name:** umass_erm 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `84b5e8a0f4434b003133a17014b73b36` 
- :material-text: **Run description:** Pseudo relevance feedback based on word embeddings and learning via neural nets. 

---
#### umass_letor_lm 
[**`Results`**](./results.md#umass_letor_lm), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_letor_lm.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_letor_lm), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_letor_lm.pdf) 

- :material-rename: **Name:** umass_letor_lm 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `b92144481041ab091d5fcc5098b2eff7` 
- :material-text: **Run description:** LambdaMart re-ranking of features derived fromtopic titles using BM25, PL2, Query likelihood and relevance model scoring. 

---
#### umass_letor_lmn 
[**`Results`**](./results.md#umass_letor_lmn), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_letor_lmn.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_letor_lmn), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_letor_lmn.pdf) 

- :material-rename: **Name:** umass_letor_lmn 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `b9fddffd0ea5d56338c8e3e462b26c9b` 
- :material-text: **Run description:** LambdaMAART ranking trained without validation set on topic titles. 

---
#### umass_letor_m 
[**`Results`**](./results.md#umass_letor_m), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_letor_m.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_letor_m), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_letor_m.pdf) 

- :material-rename: **Name:** umass_letor_m 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `46cf650b0c8de14ed9ea0fe3e13a0afc` 
- :material-text: **Run description:** MART re-ranking trained on topic titles. 

---
#### umass_maxpas150 
[**`Results`**](./results.md#umass_maxpas150), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_maxpas150.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_maxpas150), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_maxpas150.pdf) 

- :material-rename: **Name:** umass_maxpas150 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `51f2dc6f3b1eebeccb1f643ade278001` 
- :material-text: **Run description:** Max passage scoring of topic titles with passage size 150.  

---
#### umass_maxpas50 
[**`Results`**](./results.md#umass_maxpas50), [**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.umass_maxpas50.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.umass_maxpas50), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/umass_maxpas50.pdf) 

- :material-rename: **Name:** umass_maxpas50 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `a1c3aa08802769859390d87ee46c63c7` 
- :material-text: **Run description:** Max passage scoring of topic titles with passage size 50.  

---
#### UWatMDS_AFuse 
[**`Results`**](./results.md#uwatmds_afuse), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_AFuse.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_AFuse), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_AFuse.pdf) 

- :material-rename: **Name:** UWatMDS_AFuse 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/30/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `9fdab27a6ec9cf7a06c11f518814c56c` 
- :material-text: **Run description:** UWaterlooMDS team collected two sets of relevance judgements for all the 250 topics by using different TAR tools. We re-ranked the documents by using these two different judgements sets separately. Then we used the reciprocal rank fusion to fuse the two rank lists of documents to compose this run. 

---
#### UWatMDS_AUnion 
[**`Results`**](./results.md#uwatmds_aunion), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_AUnion.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_AUnion), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_AUnion.pdf) 

- :material-rename: **Name:** UWatMDS_AUnion 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/30/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `e249ccaaf00d80a7ea4bdd003b20f49e` 
- :material-text: **Run description:** UWaterlooMDS team collected two sets of relevance judgements for all the 250 topics by using different TAR tools. We re-ranked the documents by using the union of these two judgements sets. 

---
#### UWatMDS_AWgtd 
[**`Results`**](./results.md#uwatmds_awgtd), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_AWgtd.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_AWgtd), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_AWgtd.pdf) 

- :material-rename: **Name:** UWatMDS_AWgtd 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `f27d3b189d850139513597f86910d6c1` 
- :material-text: **Run description:** UWaterlooMDS team collected two sets of relevance judgements for all the 250 topics by using different TAR tools. We assigned different weights to documents according to their relevance from these two different judgements sets and trained a model. The we used the model to re-rank all the documents. 

---
#### UWatMDS_BFuse 
[**`Results`**](./results.md#uwatmds_bfuse), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_BFuse.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_BFuse), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_BFuse.pdf) 

- :material-rename: **Name:** UWatMDS_BFuse 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/30/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `b1bc0183188080347bd3564003baea55` 
- :material-text: **Run description:** UWaterlooMDS team collected three sets of relevance judgements for 50 NIST topics by using different TAR tools. We re-ranked the documents by using these three different judgements sets separately. Then we used the reciprocal rank fusion to fuse the three rank lists of documents to compose this run.  

---
#### UWatMDS_BUnion 
[**`Results`**](./results.md#uwatmds_bunion), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_BUnion.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_BUnion), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_BUnion.pdf) 

- :material-rename: **Name:** UWatMDS_BUnion 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/30/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `2b3c8935e449dd1573c88d3633e837f0` 
- :material-text: **Run description:** UWaterlooMDS team collected three sets of relevance judgements for 50 NIST topics by using different TAR tools. We re-ranked the documents by using the union of these three judgements sets.  

---
#### UWatMDS_BWgtd 
[**`Results`**](./results.md#uwatmds_bwgtd), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_BWgtd.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_BWgtd), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_BWgtd.pdf) 

- :material-rename: **Name:** UWatMDS_BWgtd 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/31/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `4a1b37497bb9e99ff84d189f1ea6a580` 
- :material-text: **Run description:** UWaterlooMDS team collected three sets of relevance judgements for 50 NIST topics by using different TAR tools. We assigned different weights to documents according to their relevance in these three different judgements sets and trained a model. The we used the model to re-rank all the documents. 

---
#### UWatMDS_HT10 
[**`Results`**](./results.md#uwatmds_ht10), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_HT10.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_HT10), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_HT10.pdf) 

- :material-rename: **Name:** UWatMDS_HT10 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/17/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `d6c8e9b641ebe1daa38d3e86354ed3a2` 
- :material-text: **Run description:** This run applied HorvitzThompson estimator to sample 10 documents from the UWatMDS_TARSv1 run. The documents were ranked by the classification scores from by ranking model and then sampled according to their reciprocal rank. The 10 selected sample were located on the top of the run. The rest documents are sorted by scores. 

---
#### UWatMDS_TARSv1 
[**`Results`**](./results.md#uwatmds_tarsv1), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_TARSv1.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_TARSv1), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_TARSv1.pdf) 

- :material-rename: **Name:** UWatMDS_TARSv1 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/17/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `4242dab5bebd124e48a098a38c9983dd` 
- :material-text: **Run description:** UWaterlooMDS team used a TAR tool along with a search interface to search and review relevant documents. We had three sets of reviewed documents: relevant, on-topic and non-relevant. We made the run by the order of relevant documents first, on-topic docs second and non-relevant docs the last. And we also built a ranking model from the reviewed documents and re-ranked each set of documents. 

---
#### UWatMDS_TARSv2 
[**`Results`**](./results.md#uwatmds_tarsv2), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_TARSv2.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_TARSv2), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_TARSv2.pdf) 

- :material-rename: **Name:** UWatMDS_TARSv2 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/18/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `53cd2bd5b7d5277090eda4d3b44309f0` 
- :material-text: **Run description:** UWatMDS_TARSv2 run also ordered the documents by rel first, on-topic second and non-rel the last as the UWatMDS_TARSv1 run. But the relative ranking of documents in each set changed since we trained the ranking model by assigning different weights to rel docs and on-topic docs. 

---
#### UWatMDS_ustudy 
[**`Results`**](./results.md#uwatmds_ustudy), [**`Participants`**](./participants.md#uwaterloomds), [**`Proceedings`**](./proceedings.md#uwaterloomds-at-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.UWatMDS_ustudy.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.UWatMDS_ustudy), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/UWatMDS_ustudy.pdf) 

- :material-rename: **Name:** UWatMDS_ustudy 
- :fontawesome-solid-user-group: **Participant:** UWaterlooMDS 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 7/30/2017 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `74e9367b0f83f2bd161f558d8b89255a` 
- :material-text: **Run description:** UWaterlooMDS team recruited real users to judge the documents by using specially designed TAR tool along with a search interface. We collected three sets of reviewed documents: highly-relevant, relevant and non-relevant for each topic. We made the run by the order of highly-relevant documents the first, relevant documents the second and non-relevant documents the last. And we also built a ranking model from the users' judgements and re-ranked each set of documents.  

---
#### WCrobust04 
[**`Results`**](./results.md#wcrobust04), [**`Participants`**](./participants.md#waterloocormack), [**`Proceedings`**](./proceedings.md#mrg-uwaterloo-and-waterloocormack-participation-in-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.WCrobust04.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.WCrobust04), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/WCrobust04.pdf) 

- :material-rename: **Name:** WCrobust04 
- :fontawesome-solid-user-group: **Participant:** WaterlooCormack 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/8/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `33768045401d9c6aa1205c3a9042cc63` 
- :material-text: **Run description:** Logistic Regression (Sofia-ML, Cornell TF-IDF features, from Total Recall BMI) trained on Robust 04 qrels. 

---
#### WCrobust0405 
[**`Results`**](./results.md#wcrobust0405), [**`Participants`**](./participants.md#waterloocormack), [**`Proceedings`**](./proceedings.md#mrg-uwaterloo-and-waterloocormack-participation-in-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.WCrobust0405.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.WCrobust0405), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/WCrobust0405.pdf) 

- :material-rename: **Name:** WCrobust0405 
- :fontawesome-solid-user-group: **Participant:** WaterlooCormack 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/8/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `791513540f70cb5cabfb1e2bf5fc777e` 
- :material-text: **Run description:** Logistic Regression (Sofia-ML, Cornell TF-IDF features, from Total Recall BMI) trained on Robust 04 plus Robust 05 (for 50 of the topics) qrels. 

---
#### WCrobust04W 
[**`Results`**](./results.md#wcrobust04w), [**`Participants`**](./participants.md#waterloocormack), [**`Proceedings`**](./proceedings.md#mrg-uwaterloo-and-waterloocormack-participation-in-the-trec-2017-common-core-track), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.WCrobust04W.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.WCrobust04W), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/WCrobust04W.pdf) 

- :material-rename: **Name:** WCrobust04W 
- :fontawesome-solid-user-group: **Participant:** WaterlooCormack 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 6/8/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `e528fcd0f1ce9b83869594cd41343248` 
- :material-text: **Run description:** Logistic Regression (Sofia-ML, Cornell TF-IDF features, from Total Recall BMI) trained on Waterloo TREC 6 qrels (iffy=relevant) for 50 topics; Robust 04 qrels for the remainder. 

---
#### webis_baseline 
[**`Results`**](./results.md#webis_baseline), [**`Participants`**](./participants.md#webis), [**`Proceedings`**](./proceedings.md#webis-at-trec-2017-open-search-and-core-tracks), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.webis_baseline.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.webis_baseline), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/webis_baseline.pdf) 

- :material-rename: **Name:** webis_baseline 
- :fontawesome-solid-user-group: **Participant:** Webis 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/2/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `ecb8f00ed8e1436971977f07a594c16f` 
- :material-text: **Run description:** BM25 over headline, abastract, first paragraph and body 

---
#### webis_baseline2 
[**`Results`**](./results.md#webis_baseline2), [**`Participants`**](./participants.md#webis), [**`Proceedings`**](./proceedings.md#webis-at-trec-2017-open-search-and-core-tracks), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.webis_baseline2.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.webis_baseline2), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/webis_baseline2.pdf) 

- :material-rename: **Name:** webis_baseline2 
- :fontawesome-solid-user-group: **Participant:** Webis 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/2/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `0df62f6e95ab8ab035745fef590addab` 
- :material-text: **Run description:** BM25 over headline, first paragraph, abstract and body. Disjunctional  

---
#### webis_reranked 
[**`Results`**](./results.md#webis_reranked), [**`Participants`**](./participants.md#webis), [**`Proceedings`**](./proceedings.md#webis-at-trec-2017-open-search-and-core-tracks), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.webis_reranked.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.webis_reranked), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/webis_reranked.pdf) 

- :material-rename: **Name:** webis_reranked 
- :fontawesome-solid-user-group: **Participant:** Webis 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/2/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `7ddfc115d56fa0d416bc678ef5d9f0b8` 
- :material-text: **Run description:** BM25 over headline, first paragraph, abstract and body. Disjunctional. Afterwords reranked according to argumentativeness 

---
#### webis_reranked2 
[**`Results`**](./results.md#webis_reranked2), [**`Participants`**](./participants.md#webis), [**`Proceedings`**](./proceedings.md#webis-at-trec-2017-open-search-and-core-tracks), [**`Input`**](https://trec.nist.gov/results/trec26/core/input.webis_reranked2.gz), [**`Summary`**](https://trec.nist.gov/results/trec26/core/summary.trec_eval.webis_reranked2), [**`Appendix`**](https://trec.nist.gov/pubs/trec26/appendices/core/webis_reranked2.pdf) 

- :material-rename: **Name:** webis_reranked2 
- :fontawesome-solid-user-group: **Participant:** Webis 
- :material-format-text: **Track:** Common Core 
- :material-calendar: **Year:** 2017 
- :material-upload: **Submission:** 8/2/2017 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** main 
- :material-fingerprint: **MD5:** `94ed1e27993bd9c0be2fd7a44ad31f5b` 
- :material-text: **Run description:** BM25 over headline, first paragraph, abstract and body. Disjunctional. Afterwords reranked according to argumentativeness 

---
