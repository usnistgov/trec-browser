# Runs - Complex Answer Retrieval 2018 

#### CG-Seq2Seq 
[**`Participants`**](./participants.md#cgnadaa) 

- :material-rename: **Run ID:** CG-Seq2Seq 
- :fontawesome-solid-user-group: **Participant:** CGNADAA 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `fdaa996116e06af873bb90516ef06e26` 
- :material-text: **Run description:** we developed a seq2seq model to map paragraphs to smaller set and then ranked the the set based on the similarity of query and the set.  

---
#### CUIS-dogeDodge 
[**`Participants`**](./participants.md#cuis), [**`Proceedings`**](./proceedings.md#cuis-team-at-trec-2018-car-track) 

- :material-rename: **Run ID:** CUIS-dogeDodge 
- :fontawesome-solid-user-group: **Participant:** CUIS 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `e471232ae2587fc103a90b5ddd203419` 
- :material-text: **Run description:** simply transform the passage ranking results in the run "CUIS-MX5", complemented by LM on 'unprocessedAllButBenchmark' 

---
#### CUIS-F150 
[**`Participants`**](./participants.md#cuis), [**`Proceedings`**](./proceedings.md#cuis-team-at-trec-2018-car-track) 

- :material-rename: **Run ID:** CUIS-F150 
- :fontawesome-solid-user-group: **Participant:** CUIS 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `5452024457e838adb9e83a751564d790` 
- :material-text: **Run description:** sequential dependence model with restricted set of bigrams (it can be regarded as a special case of the "Latent Concept Expansion" model) WikiTreeLM (or generally WikiTreeSDM) selects a sequence of sections (i.e. a path in the wiki tree) of the corresponding Wikipedia article for each candidate paragraph, which maximizes a Dirchlet prior smoothed based scoring function.  parameter tunning on benchmarkY1test automatic qrels 

---
#### CUIS-MX5 
[**`Participants`**](./participants.md#cuis), [**`Proceedings`**](./proceedings.md#cuis-team-at-trec-2018-car-track) 

- :material-rename: **Run ID:** CUIS-MX5 
- :fontawesome-solid-user-group: **Participant:** CUIS 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `ac48ac81686f22d29cbb7e306d51718e` 
- :material-text: **Run description:** SDM with restricted set of bigrams (it can be regarded as a special case of the "Latent Concept Expansion" model). WikiTreeLM (or generally, WikiTreeSDM) selects a sequence of sections of the corresponding Wikipedia article for each paragraph that maximizes a Dirchlet prior smoothed based scoring function. (consider weighted sum of concept frequencies/document lengths of each section) Entity LM: a simple entity language model.    Use TAGME for entity linking on Wikipedia 2015-10. Identified query entities not in unprocessedAllButBenchmark are removed.  

---
#### CUIS-Swift 
[**`Participants`**](./participants.md#cuis), [**`Proceedings`**](./proceedings.md#cuis-team-at-trec-2018-car-track) 

- :material-rename: **Run ID:** CUIS-Swift 
- :fontawesome-solid-user-group: **Participant:** CUIS 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `4617e95b832a13de1cc87783bc3936a7` 
- :material-text: **Run description:** simply transform the passage ranking results in the run "CUIS-MX5" 

---
#### CUIS-XTS 
[**`Participants`**](./participants.md#cuis), [**`Proceedings`**](./proceedings.md#cuis-team-at-trec-2018-car-track) 

- :material-rename: **Run ID:** CUIS-XTS 
- :fontawesome-solid-user-group: **Participant:** CUIS 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `51cc6e083db7022a7ad651d3e4cf253b` 
- :material-text: **Run description:** simply transform the passage ranking results in the run "CUIS-F150" 

---
#### DWS-UMA-EntAspBM25none 
[**`Participants`**](./participants.md#dws-uma), [**`Proceedings`**](./proceedings.md#trec-car-2018-a-simple-unsupervised-semantic-query-expansion-model) 

- :material-rename: **Run ID:** DWS-UMA-EntAspBM25none 
- :fontawesome-solid-user-group: **Participant:** DWS-UMA 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `b698617d464607f39f3356e6239beed2` 
- :material-text: **Run description:** bm25 

---
#### DWS-UMA-EntAspQLrm 
[**`Participants`**](./participants.md#dws-uma), [**`Proceedings`**](./proceedings.md#trec-car-2018-a-simple-unsupervised-semantic-query-expansion-model) 

- :material-rename: **Run ID:** DWS-UMA-EntAspQLrm 
- :fontawesome-solid-user-group: **Participant:** DWS-UMA 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `32c2b23a8bb8b7173aea022f7c90d902` 
- :material-text: **Run description:** bm25 

---
#### DWS-UMA-SemqQueryExp 
[**`Participants`**](./participants.md#dws-uma), [**`Proceedings`**](./proceedings.md#trec-car-2018-a-simple-unsupervised-semantic-query-expansion-model) 

- :material-rename: **Run ID:** DWS-UMA-SemqQueryExp 
- :fontawesome-solid-user-group: **Participant:** DWS-UMA 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `e49b68826b3d54c37ee587f915c7b052` 
- :material-text: **Run description:** Semantic Query Expansion with the k-nn of each query term + bm25 and boosting specific query terms, terms appearing on lower levels -> higher weights. We obtain the nearest neighbors from the GloVe Embedding Space. K=10 

---
#### DWS-UMA-SemqQueryExp20 
[**`Participants`**](./participants.md#dws-uma), [**`Proceedings`**](./proceedings.md#trec-car-2018-a-simple-unsupervised-semantic-query-expansion-model) 

- :material-rename: **Run ID:** DWS-UMA-SemqQueryExp20 
- :fontawesome-solid-user-group: **Participant:** DWS-UMA 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `35e3168d2be579030cb4c32c93859faa` 
- :material-text: **Run description:** Semantic Query Expansion with the k-nn of each query term + bm25 and boosting specific query terms, terms appearing on lower levels -> higher weights. We obtain the nearest neighbors from the GloVe Embedding Space. K=20 

---
#### DWS-UMA-SemqQueryExp30 
[**`Participants`**](./participants.md#dws-uma), [**`Proceedings`**](./proceedings.md#trec-car-2018-a-simple-unsupervised-semantic-query-expansion-model) 

- :material-rename: **Run ID:** DWS-UMA-SemqQueryExp30 
- :fontawesome-solid-user-group: **Participant:** DWS-UMA 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `c1bf07649452d4a5d8007cb81977bb98` 
- :material-text: **Run description:** Semantic Query Expansion with the k-nn of each query term + bm25 and boosting specific query terms, terms appearing on lower levels -> higher weights. We obtain the nearest neighbors from the GloVe Embedding Space. K=20 

---
#### entityEmbedLambdaMart 
[**`Participants`**](./participants.md#umass), [**`Proceedings`**](./proceedings.md#umass-at-trec-2018-car-common-core-and-news-tracks) 

- :material-rename: **Run ID:** entityEmbedLambdaMart 
- :fontawesome-solid-user-group: **Participant:** UMass 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `80b0ab0ad07ab19842336724c1559b63` 
- :material-text: **Run description:** In this run, we want to investigate entity embeddings in the task of passage ranking. Each query and each document has an entity representation. Documents are represented as the average of entity embeddings within the passage.  Queries have different representations including the complete outline and the subqueries. The similarity between query representations and document representation is used as features for learning to rank model.  

---
#### guir 
[**`Participants`**](./participants.md#guir), [**`Proceedings`**](./proceedings.md#pacrr-gated-expansion-for-trec-car-2018) 

- :material-rename: **Run ID:** guir 
- :fontawesome-solid-user-group: **Participant:** GUIR 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `fab753dd5c9a0fc6a87bb5aac8ba447d` 
- :material-text: **Run description:** PACRR neural ranking architecture modified with heading independence and heading frequency contextual vector 

---
#### guir-exp 
[**`Participants`**](./participants.md#guir), [**`Proceedings`**](./proceedings.md#pacrr-gated-expansion-for-trec-car-2018) 

- :material-rename: **Run ID:** guir-exp 
- :fontawesome-solid-user-group: **Participant:** GUIR 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `d33c789c27f9c9b35a688a6c114b2f64` 
- :material-text: **Run description:** PACRR neural ranking architecture modified with heading independence, heading frequency contextual vector, and expanded query terms by heading 

---
#### NYU-L 
[**`Participants`**](./participants.md#nyu-dl), [**`Proceedings`**](./proceedings.md#new-york-university-at-trec-2018-complex-answer-retrieval-track) 

- :material-rename: **Run ID:** NYU-L 
- :fontawesome-solid-user-group: **Participant:** NYU-DL 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/14/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `402dc7d4b968eea12f4619e41b02f2e7` 
- :material-text: **Run description:** Lucene is the underlying retrieval engine. We train 20 query reformulators from Nogueira and Cho (2017) on random disjoint partitions of the training set. For each query, each reformulator produces a list of ranked documents. We re-rank the union of these 20 lists using a simple ranking model that scores each query-document pair using a feed-forward neural network whose input is the concatenation of the average word embeddings of the query and document. To further improve the performance of the system, we train an ensemble of 10 ranking models whose network architectures are randomly chosen. For each query, we re-rank the union of the 10 lists produced by the 10 ranking models using the best ranking model in the ensemble. 

---
#### NYU-M 
[**`Participants`**](./participants.md#nyu-dl), [**`Proceedings`**](./proceedings.md#new-york-university-at-trec-2018-complex-answer-retrieval-track) 

- :material-rename: **Run ID:** NYU-M 
- :fontawesome-solid-user-group: **Participant:** NYU-DL 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `68fdf25345c038d8714052486514a204` 
- :material-text: **Run description:** Lucene is the underlying retrieval engine. We train 20 query reformulators from Nogueira and Cho (2017) on random disjoint partitions of the training set. For each query, each reformulator produces a list of ranked documents. We re-rank the union of these 20 lists using a simple ranking model that scores each query-document pair using a feed-forward neural network whose input is the concatenation of the average word embeddings of the query and document. To further improve the performance of the system, we train an ensemble of 5 ranking models whose network architectures are randomly chosen. For each query, we re-rank the union of the 5 lists produced by the 5 ranking models using the best ranking model in the ensemble.  

---
#### NYU-XL 
[**`Participants`**](./participants.md#nyu-dl), [**`Proceedings`**](./proceedings.md#new-york-university-at-trec-2018-complex-answer-retrieval-track) 

- :material-rename: **Run ID:** NYU-XL 
- :fontawesome-solid-user-group: **Participant:** NYU-DL 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `14d686935b9e8097344563d4c9bc4344` 
- :material-text: **Run description:** Lucene is the underlying retrieval engine. We train 20 query reformulators from Nogueira and Cho (2017) on random disjoint partitions of the training set. For each query, each reformulator produces a list of ranked documents. We re-rank the union of these 20 lists using a simple ranking model that scores each query-document pair using a feed-forward neural network whose input is the concatenation of the average word embeddings of the query and document. To further improve the performance of the system, we train an ensemble of 11 ranking models whose network architectures are randomly chosen. For each query, we re-rank the union of the 11 lists produced by the 11 ranking models using the best ranking model in the ensemble.  

---
#### UNH-e-graph 
[**`Participants`**](./participants.md#trema-unh), [**`Proceedings`**](./proceedings.md#trema-unh-at-trec-2018-complex-answer-retrieval-and-news-track) 

- :material-rename: **Run ID:** UNH-e-graph 
- :fontawesome-solid-user-group: **Participant:** trema-unh 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `b63c0cabb4e74daadc04d6a6fae0a7c3` 
- :material-text: **Run description:** Trained on benchmarkt1train  - tree.entity.qrels extended entity links with DBpedia Spotlight (retaining only links to entities that already have a link somewhere on the article) best variant selected on benchmarkY1test - extended tree.entity.qrels Different indexes for retrieving entities and paragraphs are built from allButBenchmark and paragraphCorpus. Rankings of entities provide a feature vector for nodes, Rankings of paragraphs provide a feature vector for edges.  A specialized Learn-to-walk algorithm is used to train how node and edge features are best combined to obtain best degree centrality rankings. The Learn-to-walk is trained with mini-batched coordinate ascent to optimize MAP of entity rankings  

---
#### UNH-e-L2R 
[**`Participants`**](./participants.md#trema-unh), [**`Proceedings`**](./proceedings.md#trema-unh-at-trec-2018-complex-answer-retrieval-and-news-track) 

- :material-rename: **Run ID:** UNH-e-L2R 
- :fontawesome-solid-user-group: **Participant:** trema-unh 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `934d7ca9a1514d3e72d11326a1eeec41` 
- :material-text: **Run description:** Four types of indices were made using Lucene v7.2.1: aspect, entity, page and paragraph.For each index, features were calculated using various combination of query-level,retrieval-model, expansion-model,and analyzer as follows: query-level: section path, all retrieval-model: BM25, Query Liklihood expansion-model: Entity Context Model                                                                                                                        analyzer: english Different indexes for retrieving entities and paragraphs are built from allButBenchmark and paragraphCorpus to obtain 10 rankings over Wikipages or paragraphs. Using the entity context model, we build entity relevance models (rankings over entity ids) from the pages/paragraphs, which are used as a ranking of entities. The particular run files are created by using a section path query to retrieve from these indices with BM25 and Querylikelihood. Additionally, paragraphs are retrieved by building a query from all headings of the topic page. These were then combined using a learning-to-rank and a combined ranking was obtained. This model is trained on benchmarkY1-train, using the document tree ground truth. The rankings were annotated with the highest ranked paragraph from an additional paragraph ranking as described below. PARAGRAPH FEATURES Fielded BM25: Weighted combination of unigram, bigram, and windowed bigram likelihood from the query to the document using BM25 algorithm. SDM: Standard SDM model Weighted Section Fielded BM25: Weighted combination of the leaf of a heading and the entire heading. Both scored with Fielded BM25. Expanded Bigrams: Using the rank score obtained from Fielded BM25, create a distribution over likely bigrams given unigrams in the query. Select the top 5 bigrams, add them to the original query, and rescore uses Fielded BM25. ENTITY -> PARAGRAPH FEATURES The following are entity features (scoring relevance of entity given query) that have been turned into paragraph features. Paragraphs are scored by integrating over the ranking scores of entities that they link to, given the entity feature. Link Freqeuncy: Each entity is scored according to the frequency that it is linked to by the candidate document (uses entity links). The following stats are parsed from v2.1 unprocessedAllButBenchmark: Categories: Unigram model of the enwiki categories of an entity. Entity Disambiguations: Unigram model of the disambiguation links of an entity Entity Inlinks: Unigram model of the disambiguation inlinks of an entity Entity Outlinks: Unigram model of the disambiguation outlinks of an entity Entity Redirects: Unigram model of the disambiguation redirect links of an entity Entity Sections: Unigram model of the section paths contained in the entity page. In addition, an "entity context" model is used. For each paragraph that an entity occurs (within the unprocessedAllButBenchmark v2.1), the surrounding unigrams, bigrams, and windowed bigrams are indexed. These -grams, along with the entity's name, are stored as pseudodocuments in an "entity context index" to be queried. Context Unigram: Query is tokenized into unigrams and used to query entity context database. Context  Bigram: Query is tokenized into bigrams and used to query entity context database. Context Windowed Bigram: Query is tokenized into windowed bigram and used to query entity context database. TRAINING This model is trained on benchmarkY1-train, using both the document tree qrels and the entity tree qrels (because the document features can be transformed into entity features and vice versa, this allows us to use both training examples).  

---
#### UNH-e-mixed 
[**`Participants`**](./participants.md#trema-unh), [**`Proceedings`**](./proceedings.md#trema-unh-at-trec-2018-complex-answer-retrieval-and-news-track) 

- :material-rename: **Run ID:** UNH-e-mixed 
- :fontawesome-solid-user-group: **Participant:** trema-unh 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `69a532aa73399fec1e636cfe30a13ac3` 
- :material-text: **Run description:** Creating entity runs, then annotating entities with highest ranked paragraphs in a secondary ranking that contain this entity. Entity Ranking: ENTITY FEATURES Link Frequency: Each entity is scored according to the frequency that it is linked to by the candidate paragraphs (uses entity links). The following stats are parsed from v2.1 unprocessedAllButBenchmark: Categories: Unigram model of the enwiki categories of an entity. Entity Disambiguations: Unigram model of the disambiguation links of an entity Entity Inlinks: Unigram model of the disambiguation inlinks of an entity Entity Outlinks: Unigram model of the disambiguation outlinks of an entity Entity Redirects: Unigram model of the disambiguation redirect links of an entity Entity Sections: Unigram model of the section paths contained in the entity page. In addition, an "entity context" model is used.  For each paragraph that an entity occurs (within the unprocessedAllButBenchmark v2.1), the surrounding unigrams, bigrams, and windowed bigrams are indexed. These -grams, along with the entity's name, are stored as pseudodocuments in an "entity context index" to be queried. Context Unigram: Query is tokenized into unigrams and used to query entity context database. Context  Bigram: Query is tokenized into bigrams and used to query entity context database. Context Windowed Bigram: Query is tokenized into windowed bigram and used to query entity context database. PARAGRAPH -> ENTITY FEATURES The following are features that score the relevance of a paragraph given a query. These are turned into entity features by integrating over the scores of each paragraph that an entity links to. Fielded BM25: Weighted combination of unigram, bigram, and windowed bigram likelihood from the query to the document using BM25 algorithm. SDM: Standard SDM model Weighted Section Fielded BM25: Weighted combination of the leaf of a heading and the entire heading. Both scored with Fielded BM25. Expanded Bigrams: Using the rank score obtained from Fielded BM25, create a distribution over likely bigrams given unigrams in the query. Select the top 5 bigrams, add them to the original query, and rescore uses Fielded BM25. TRAINING This model is trained on benchmarkY1-train, using the document tree qrels. Paragraph Ranking: Fielded BM25: Weighted combination of unigram, bigram, and windowed bigram likelihood from the query to the document using BM25 algorithm. SDM: Standard SDM model Weighted Section Fielded BM25: Weighted combination of the leaf of a heading and the entire heading. Both scored with Fielded BM25. Expanded Bigrams: Using the rank score obtained from Fielded BM25, create a distribution over likely bigrams given unigrams in the query. Select the top 5 bigrams, add them to the original query, and rescore uses Fielded BM25. ENTITY -> PARAGRAPH FEATURES The following are entity features (scoring relevance of entity given query) that have been turned into paragraph features. Paragraphs are scored by integrating over the ranking scores of entities that they link to, given the entity feature. Link Freqeuncy: Each entity is scored according to the frequency that it is linked to by the candidate document (uses entity links). The following stats are parsed from v2.1 unprocessedAllButBenchmark: Categories: Unigram model of the enwiki categories of an entity. Entity Disambiguations: Unigram model of the disambiguation links of an entity Entity Inlinks: Unigram model of the disambiguation inlinks of an entity Entity Outlinks: Unigram model of the disambiguation outlinks of an entity Entity Redirects: Unigram model of the disambiguation redirect links of an entity Entity Sections: Unigram model of the section paths contained in the entity page. In addition, an "entity context" model is used.  For each paragraph that an entity occurs (within the unprocessedAllButBenchmark v2.1), the surrounding unigrams, bigrams, and windowed bigrams are indexed. These -grams, along with the entity's name, are stored as pseudodocuments in an "entity context index" to be queried. Context Unigram: Query is tokenized into unigrams and used to query entity context database. Context  Bigram: Query is tokenized into bigrams and used to query entity context database. Context Windowed Bigram: Query is tokenized into windowed bigram and used to query entity context database. TRAINING This model is trained on benchmarkY1-train, using both the document tree qrels and the entity tree qrels (because the document features can be transformed into entity features and vice versa, this allows us to use both training examples).  

---
#### UNH-p-l2r 
[**`Participants`**](./participants.md#trema-unh), [**`Proceedings`**](./proceedings.md#trema-unh-at-trec-2018-complex-answer-retrieval-and-news-track) 

- :material-rename: **Run ID:** UNH-p-l2r 
- :fontawesome-solid-user-group: **Participant:** trema-unh 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `908f6c35b5f150cda1885280551f2138` 
- :material-text: **Run description:** Features: - BM25 (no expansion) - BM25 with RM3 expansion - Query likelihood (no expansion) - Query likelihood with RM3 expansion They are combined using learning to rank; RankLib with coordinate ascent optimized on map method is trained with data provided as benchmarkY1train External resources: Ranking: BM25, query likelihood (Lucene 7) Description: five fold cross validation is used with learning to rank to produce the training map. the full BY1train dataset is used with learning to rank to produce the runs on BY1test and BY2test. also in each run files used for combining using l2r, the retrieval scores are normalized which means for each query, scores are divided by the maximum score for that query.  

---
#### UNH-p-mixed 
[**`Participants`**](./participants.md#trema-unh), [**`Proceedings`**](./proceedings.md#trema-unh-at-trec-2018-complex-answer-retrieval-and-news-track) 

- :material-rename: **Run ID:** UNH-p-mixed 
- :fontawesome-solid-user-group: **Participant:** trema-unh 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `25b4127d2684a8c180d86f510dbe42cc` 
- :material-text: **Run description:** PARAGRAPH FEATURES Fielded BM25: Weighted combination of unigram, bigram, and windowed bigram likelihood from the query to the document using BM25 algorithm. SDM: Standard SDM model Weighted Section Fielded BM25: Weighted combination of the leaf of a heading and the entire heading. Both scored with Fielded BM25. Expanded Bigrams: Using the rank score obtained from Fielded BM25, create a distribution over likely bigrams given unigrams in the query. Select the top 5 bigrams, add them to the original query, and rescore uses Fielded BM25. ENTITY -> PARAGRAPH FEATURES The following are entity features (scoring relevance of entity given query) that have been turned into paragraph features. Paragraphs are scored by integrating over the ranking scores of entities that they link to, given the entity feature. Link Freqeuncy: Each entity is scored according to the frequency that it is linked to by the candidate document (uses entity links). The following stats are parsed from v2.1 unprocessedAllButBenchmark: Categories: Unigram model of the enwiki categories of an entity. Entity Disambiguations: Unigram model of the disambiguation links of an entity Entity Inlinks: Unigram model of the disambiguation inlinks of an entity Entity Outlinks: Unigram model of the disambiguation outlinks of an entity Entity Redirects: Unigram model of the disambiguation redirect links of an entity Entity Sections: Unigram model of the section paths contained in the entity page. In addition, an "entity context" model is used.  For each paragraph that an entity occurs (within the unprocessedAllButBenchmark v2.1), the surrounding unigrams, bigrams, and windowed bigrams are indexed. These -grams, along with the entity's name, are stored as pseudodocuments in an "entity context index" to be queried. Context Unigram: Query is tokenized into unigrams and used to query entity context database. Context  Bigram: Query is tokenized into bigrams and used to query entity context database. Context Windowed Bigram: Query is tokenized into windowed bigram and used to query entity context database. TRAINING This model is trained on benchmarkY1-train, using both the document tree qrels and the entity tree qrels (because the document features can be transformed into entity features and vice versa, this allows us to use both training examples).  

---
#### UNH-p-sdm 
[**`Participants`**](./participants.md#trema-unh), [**`Proceedings`**](./proceedings.md#trema-unh-at-trec-2018-complex-answer-retrieval-and-news-track) 

- :material-rename: **Run ID:** UNH-p-sdm 
- :fontawesome-solid-user-group: **Participant:** trema-unh 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `6ba9206e60b9ac4359bb28db663611b7` 
- :material-text: **Run description:** PARAGRAPH FEATURES Fielded BM25: Weighted combination of unigram, bigram, and windowed bigram likelihood from the query to the document using BM25 algorithm. TRAINING This model is trained on benchmarkY1-train, using the document tree qrels.  

---
#### uog-heading-rh-sdm 
[**`Participants`**](./participants.md#uogtr) 

- :material-rename: **Run ID:** uog-heading-rh-sdm 
- :fontawesome-solid-user-group: **Participant:** uogTr 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `a845eb3c450e79e3d93aaaa8cbd0e5aa` 
- :material-text: **Run description:** A baseline sequential dependence model run that uses the root and leaf heading, ignoring intermediate nodes. 

---
#### uog-heading-rh-sdm-ent 
[**`Participants`**](./participants.md#uogtr) 

- :material-rename: **Run ID:** uog-heading-rh-sdm-ent 
- :fontawesome-solid-user-group: **Participant:** uogTr 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `2808d5546c6d646135e116ad003d4a16` 
- :material-text: **Run description:** A simple baseline run that uses SDM on the root and leaf heading. 

---
#### uog-linear-ltr-hier 
[**`Participants`**](./participants.md#uogtr) 

- :material-rename: **Run ID:** uog-linear-ltr-hier 
- :fontawesome-solid-user-group: **Participant:** uogTr 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `1e1fe92542e04177974f549ae5b95d14` 
- :material-text: **Run description:** A feature combination model that includes baseline retrieval methods (QL, SDM, RM3), as well as entity-based query expansion features from PRF on AllButBench and from query entity linking. 

---
#### uog-linear-ltr-hier-ent 
[**`Participants`**](./participants.md#uogtr) 

- :material-rename: **Run ID:** uog-linear-ltr-hier-ent 
- :fontawesome-solid-user-group: **Participant:** uogTr 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `624a8555fcb76d9e256db7e3c65ee7f7` 
- :material-text: **Run description:** A feature combination model that includes baseline retrieval methods (QL, SDM, RM3), as well as entity-based query expansion features from PRF on AllButBench and from query entity linking. 

---
#### uog-linear-raw-expansion 
[**`Participants`**](./participants.md#uogtr) 

- :material-rename: **Run ID:** uog-linear-raw-expansion 
- :fontawesome-solid-user-group: **Participant:** uogTr 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/16/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `df13759448734d61ea0b0752b9626b0d` 
- :material-text: **Run description:** This is a full entity expansion run without a working set. It converts the learned expansion query weights into a galago weighted query and runs this against the full paragraph index. 

---
#### uog-paragraph-rf-ent 
[**`Participants`**](./participants.md#uogtr) 

- :material-rename: **Run ID:** uog-paragraph-rf-ent 
- :fontawesome-solid-user-group: **Participant:** uogTr 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** entities 
- :material-fingerprint: **MD5:** `b76deef4c911de29f06d2d17091fd8c2` 
- :material-text: **Run description:** This is a entity relevance feedback run that is performed on top of uog-linear-ltr-hier.  

---
#### UTDHLTRI2 
[**`Participants`**](./participants.md#utdhltri), [**`Proceedings`**](./proceedings.md#utd-hltri-at-trec-2018-complex-answer-retrieval-track) 

- :material-rename: **Run ID:** UTDHLTRI2 
- :fontawesome-solid-user-group: **Participant:** UTDHLTRI 
- :material-format-text: **Track:** Complex Answer Retrieval 
- :material-calendar: **Year:** 2018 
- :material-upload: **Submission:** 8/15/2018 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-text-search: **Task:** passages 
- :material-fingerprint: **MD5:** `28f62f9cc17ed1f0f83ded367b969410` 
- :material-text: **Run description:** This method uses an initial BM-25 search followed by a neural re-ranking system using word embeddings, a cosine similarity matrix, and IR features. 

---
