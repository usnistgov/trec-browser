# Runs - Conversational Assistance 2020 

#### ASCFDA_baseline 
[**`Results`**](./results.md#ascfda_baseline), [**`Participants`**](./participants.md#ascfda), [**`Proceedings`**](./proceedings.md#query-expansion-with-semantic-based-ellipsis-reduction-for-conversational-ir), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.ASCFDA_baseline.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.ASCFDA_baseline), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/ASCFDA_baseline.pdf) 

- :material-rename: **Run ID:** ASCFDA_baseline 
- :fontawesome-solid-user-group: **Participant:** ASCFDA 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `bbfbb6ae59ac12f900b2a614e8020dc1` 
- :material-text: **Run description:** Our reranking model, based on T5, is trained with MS MARCO dataset. 

---
#### ASCFDA_d2q_emb 
[**`Results`**](./results.md#ascfda_d2q_emb), [**`Participants`**](./participants.md#ascfda), [**`Proceedings`**](./proceedings.md#query-expansion-with-semantic-based-ellipsis-reduction-for-conversational-ir), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.ASCFDA_d2q_emb.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.ASCFDA_d2q_emb), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/ASCFDA_d2q_emb.pdf) 

- :material-rename: **Run ID:** ASCFDA_d2q_emb 
- :fontawesome-solid-user-group: **Participant:** ASCFDA 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `83ad133ce9caab9c45453798f3808a09` 
- :material-text: **Run description:** Our reranking model, based on T5, is trained with MS MARCO dataset. We use Doc2Query and BM25 to expand the raw utterances, re-retrieve the passages and rerank them by the T5-based reranking method. 

---
#### ASCFDA_qa 
[**`Results`**](./results.md#ascfda_qa), [**`Participants`**](./participants.md#ascfda), [**`Proceedings`**](./proceedings.md#query-expansion-with-semantic-based-ellipsis-reduction-for-conversational-ir), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.ASCFDA_qa.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.ASCFDA_qa), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/ASCFDA_qa.pdf) 

- :material-rename: **Run ID:** ASCFDA_qa 
- :fontawesome-solid-user-group: **Participant:** ASCFDA 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `72d5efbb2a40d6989fe5e8f8e971251c` 
- :material-text: **Run description:** Our reranking model, based on T5, is trained with MS MARCO dataset. We use both manual canonical results and topic-turns as inputs of T5-QA model, and use the previous output for query expansion. 

---
#### ASCFDA_rm3 
[**`Results`**](./results.md#ascfda_rm3), [**`Participants`**](./participants.md#ascfda), [**`Proceedings`**](./proceedings.md#query-expansion-with-semantic-based-ellipsis-reduction-for-conversational-ir), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.ASCFDA_rm3.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.ASCFDA_rm3), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/ASCFDA_rm3.pdf) 

- :material-rename: **Run ID:** ASCFDA_rm3 
- :fontawesome-solid-user-group: **Participant:** ASCFDA 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `ae393ff447b50158e074b8e180e823cf` 
- :material-text: **Run description:** Our reranking model, based on T5, is trained with MS MARCO dataset. We use RM3 to expand the raw utterances, re-retrieve the passages and rerank them by the T5-based reranking method. 

---
#### AUTO_BERT100 
[**`Results`**](./results.md#auto_bert100), [**`Participants`**](./participants.md#nova-search), [**`Proceedings`**](./proceedings.md#nova-at-trec-2020-conversational-assistance-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.AUTO_BERT100.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.AUTO_BERT100), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/AUTO_BERT100.pdf) 

- :material-rename: **Run ID:** AUTO_BERT100 
- :fontawesome-solid-user-group: **Participant:** nova-search 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `5a909880b123209f5ad18d632335cffb` 
- :material-text: **Run description:** To maintain the conversational context across turns we used the automatically rewritten queries from CAsT Y2. Initial retrieval is done with the rewritten query using a QL model with LMD smoothing. After this we perform reranking using a BERT model finetuned on the MS MARCO dataset for the passage reranking task. 

---
#### AUTO_T5_RRF 
[**`Results`**](./results.md#auto_t5_rrf), [**`Participants`**](./participants.md#nova-search), [**`Proceedings`**](./proceedings.md#nova-at-trec-2020-conversational-assistance-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.AUTO_T5_RRF.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.AUTO_T5_RRF), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/AUTO_T5_RRF.pdf) 

- :material-rename: **Run ID:** AUTO_T5_RRF 
- :fontawesome-solid-user-group: **Participant:** nova-search 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `1c4edb482d4d9e20fb14bd39084007d1` 
- :material-text: **Run description:** To maintain the conversational context across turns we use two query rewriting methods. The first one is a T5 model finetuned on the conversational query rewriting task using the CANARD dataset. The input to the T5 model is the concatenation of the current query and all of the previous queries, and the output of the model is the query rewritten for that turn. Our second method uses the automatically rewritten queries from CAsT Y2. Initial retrieval is done with both rewritten queries using a QL model with LMD smoothing. After this we perform reranking using a BERT model finetuned on the MS MARCO dataset for the passage reranking task. To obtain the final ranking we use RRF to fuse the BERT reranked results from both queries. 

---
#### baselineQR 
[**`Results`**](./results.md#baselineqr), [**`Participants`**](./participants.md#uvailps), [**`Proceedings`**](./proceedings.md#leveraging-query-resolution-and-reading-comprehension-for-conversational-passage-retrieval), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.baselineQR.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.baselineQR), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/baselineQR.pdf) 

- :material-rename: **Run ID:** baselineQR 
- :fontawesome-solid-user-group: **Participant:** UvA.ILPS 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `b8533fb775842868c9926b2732c6bda3` 
- :material-text: **Run description:** PR: BM25 Anserini + BERT for passage ranking combined with the score from Roberta fine-tuned on MRQA formulated as a binary classification task (paragraph contains answer or doesn't) 

---
#### castur_albert 
[**`Results`**](./results.md#castur_albert), [**`Participants`**](./participants.md#usi), [**`Proceedings`**](./proceedings.md#extending-the-use-of-previous-relevant-utterances-for-response-ranking-in-conversational-search), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.castur_albert.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.castur_albert), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/castur_albert.pdf) 

- :material-rename: **Run ID:** castur_albert 
- :fontawesome-solid-user-group: **Participant:** USI 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `bc3171b367ac69ee318338db5f26d3f1` 
- :material-text: **Run description:** First, we perform query expansion by concatenating relevant raw utterances from history with the current raw one, based on an output of a model trained on CAsTUR dataset (Aliannejadi et al., 2020). We perform initial retrieval with Anserini Query Likelihood with Dirichlet smoothing (default parameters). Then we re-rank the passages with Albert, which is pretrained on MS MARCO passage retrieval task.  

---
#### duth 
[**`Results`**](./results.md#duth), [**`Participants`**](./participants.md#duth), [**`Proceedings`**](./proceedings.md#duth-at-trec-2020-conversational-assistance-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.duth.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.duth), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/duth.pdf) 

- :material-rename: **Run ID:** duth 
- :fontawesome-solid-user-group: **Participant:** DUTH 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `c3f4edc2c1c8b3d9e3b591bda511e04a` 
- :material-text: **Run description:** First, we run a linguistic analysis of the given turn (query) and perform coreference resolution (AllenNLP) from previous queries, POS tagging (SpaCy), and named entity recognition. Then we rewrite the queries and perform retrieval (indri) by using weights. Nouns, verbs, adjectives, and adverbs from the current query weight 1.0 and nouns, adjectives, and adverbs from the previous 2 turns (queries) weight 0.5 and 0.25 respectively. No reranking is performed. 

---
#### duth_arq 
[**`Results`**](./results.md#duth_arq), [**`Participants`**](./participants.md#duth), [**`Proceedings`**](./proceedings.md#duth-at-trec-2020-conversational-assistance-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.duth_arq.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.duth_arq), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/duth_arq.pdf) 

- :material-rename: **Run ID:** duth_arq 
- :fontawesome-solid-user-group: **Participant:** DUTH 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `78ca7ce3c379c5cb969e4cbec53a240a` 
- :material-text: **Run description:** We used the automatically rewritten utterances (queries) from CAsT Y2. First, we run a linguistic analysis of the given turn (query) and perform coreference resolution (AllenNLP) from previous queries, POS tagging (SpaCy), and named entity recognition. Then we rewrite the queries and perform retrieval (indri) by using weights. Nouns, verbs, adjectives, and adverbs from the current query weight 1.0 and nouns, adjectives, and adverbs from the previous 2 turns (queries) weight 0.5 and 0.25 respectively. No reranking is performed. 

---
#### duth_manual 
[**`Results`**](./results.md#duth_manual), [**`Participants`**](./participants.md#duth), [**`Proceedings`**](./proceedings.md#duth-at-trec-2020-conversational-assistance-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.duth_manual.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.duth_manual), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/duth_manual.pdf) 

- :material-rename: **Run ID:** duth_manual 
- :fontawesome-solid-user-group: **Participant:** DUTH 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `9e0bc5b9ad69e0dd6cb7b6adbf747a7d` 
- :material-text: **Run description:** We used the manually rewritten utterances from CAsT Y2. We rewrite the queries and perform retrieval (indri) by using weights. Nouns, verbs, adjectives, and adverbs from the current query weight 1.0 and nouns, adjectives, and adverbs from the previous 2 turns (queries) weight 0.5 and 0.25 respectively. No reranking is performed. 

---
#### grill_bmDuo 
[**`Results`**](./results.md#grill_bmduo), [**`Participants`**](./participants.md#grill), [**`Proceedings`**](./proceedings.md#glasgow-representation-and-information-learning-lab-grill-at-the-conversational-assistance-track-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.grill_bmDuo.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.grill_bmDuo), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/grill_bmDuo.pdf) 

- :material-rename: **Run ID:** grill_bmDuo 
- :fontawesome-solid-user-group: **Participant:** grill 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `753011a389d33600c4f56b5bac33ba95` 
- :material-text: **Run description:** This run uses the manually resolved queries as well as provided canonical results. No additional rewriting is performed. Base retrieval using a tuned BM25 model.  The resulting first pass retrieval is reranked with Mono + Duo BERT trained on MS MARCO.  

---
#### grill_ctxDuo 
[**`Results`**](./results.md#grill_ctxduo), [**`Participants`**](./participants.md#grill), [**`Proceedings`**](./proceedings.md#glasgow-representation-and-information-learning-lab-grill-at-the-conversational-assistance-track-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.grill_ctxDuo.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.grill_ctxDuo), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/grill_ctxDuo.pdf) 

- :material-rename: **Run ID:** grill_ctxDuo 
- :fontawesome-solid-user-group: **Participant:** grill 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `62c7ad7752f32e3d34888a2285e4b4c6` 
- :material-text: **Run description:** This run uses the manually resolved queries as well as provided canonical results. No additional rewriting is performed. Base retrieval is performed using the sequential dependence model on the current turn combined with a weighted QL model (using first turn and previous turn) as well as feedback from the canonical result provided for the previous turn.  The resulting first pass retrieval is reranked with Mono + Duo BERT trained on MS MARCO.  

---
#### grill_duoBART 
[**`Results`**](./results.md#grill_duobart), [**`Participants`**](./participants.md#grill), [**`Proceedings`**](./proceedings.md#glasgow-representation-and-information-learning-lab-grill-at-the-conversational-assistance-track-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.grill_duoBART.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.grill_duoBART), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/grill_duoBART.pdf) 

- :material-rename: **Run ID:** grill_duoBART 
- :fontawesome-solid-user-group: **Participant:** grill 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `0323ac5e0056f2f0c23cb1e84f3f3789` 
- :material-text: **Run description:** This run uses the automatic rewrites as well as provided canonical results. Additional rewriting is performed using a BART rewriter trained on Y1. BM25 turned for the first pass. The resulting first pass retrieval is reranked with Mono + Duo BERT trained on MS MARCO.  

---
#### grill_fuseDuo 
[**`Results`**](./results.md#grill_fuseduo), [**`Participants`**](./participants.md#grill), [**`Proceedings`**](./proceedings.md#glasgow-representation-and-information-learning-lab-grill-at-the-conversational-assistance-track-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.grill_fuseDuo.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.grill_fuseDuo), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/grill_fuseDuo.pdf) 

- :material-rename: **Run ID:** grill_fuseDuo 
- :fontawesome-solid-user-group: **Participant:** grill 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `2f4513dfacdb7865a9c62d85689db63f` 
- :material-text: **Run description:** This run uses the automatic rewrites as well as provided canonical results. Additional rewriting is performed using a BART rewriter trained on Y1 and a simple fusion (combination of both rewrites is used). Base retrieval is performed using the sequential dependence model on the current turn combined with a weighted QL model (using first turn and previous turn (with rewrites)) as well as feedback from the canonical result provided for the previous turn. The resulting first pass retrieval is reranked with Mono + Duo BERT trained on MS MARCO.  

---
#### h2oloo_RUN1 
[**`Results`**](./results.md#h2oloo_run1), [**`Participants`**](./participants.md#h2oloo), [**`Proceedings`**](./proceedings.md#trec-2020-notebook-cast-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.h2oloo_RUN1.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.h2oloo_RUN1), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/h2oloo_RUN1.pdf) 

- :material-rename: **Run ID:** h2oloo_RUN1 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** canonical 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `c5cd69f8a3692640b02bbf81bf108027` 
- :material-text: **Run description:** For each turn, we use word matching rule-base to extract the sentence from its previous manual canonical response (passage). We then concat the extracted sentence and current query to conduct query reformulation. At next turn, we use the historical reformulated query as context combined with the extracted sentence from the last manual canonical response to conduct query reformulation. In this run, we also fuse the result between retrieval and rerank stage. 

---
#### h2oloo_RUN2 
[**`Results`**](./results.md#h2oloo_run2), [**`Participants`**](./participants.md#h2oloo), [**`Proceedings`**](./proceedings.md#trec-2020-notebook-cast-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.h2oloo_RUN2.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.h2oloo_RUN2), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/h2oloo_RUN2.pdf) 

- :material-rename: **Run ID:** h2oloo_RUN2 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** canonical 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `c69a9576b4dcf76ffff4f1bbb694fb12` 
- :material-text: **Run description:** For each turn, we use word matching rule-base to extract the sentence from its previous manual canonical response (passage). We then concat the extracted sentence and current query to conduct query reformulation. At next turn, we use the historical reformulated query as context combined with the extracted sentence from the last manual canonical response to conduct query reformulation. As for retrieval, we combine BERT embedding search and BM25 as first-stage retrieval and use T5 3B for the second stage reranker. 

---
#### h2oloo_RUN3 
[**`Results`**](./results.md#h2oloo_run3), [**`Participants`**](./participants.md#h2oloo), [**`Proceedings`**](./proceedings.md#trec-2020-notebook-cast-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.h2oloo_RUN3.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.h2oloo_RUN3), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/h2oloo_RUN3.pdf) 

- :material-rename: **Run ID:** h2oloo_RUN3 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `8e1aa44e0df21a3f1961c53af9a94071` 
- :material-text: **Run description:** This run uses CANARD dataset to facilitate two tasks regarding conversational query reformulation. Specifically, for the first task, we use concated raw historical queries as context to infer human resolved queries. For the second task, we concated human resolved historical queries in each dialogue. After that, we facilitate Masked Language Model on these dialogue context. We finetune the T5 model one these two task jointly and conduct conversational query reformulation as the first task with CAsT's raw queries with historical context. As for retrieval, we combine BERT embedding search and BM25 as first-stage retrieval and use T5 3B for the second stage reranker.   

---
#### h2oloo_RUN4 
[**`Results`**](./results.md#h2oloo_run4), [**`Participants`**](./participants.md#h2oloo), [**`Proceedings`**](./proceedings.md#trec-2020-notebook-cast-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.h2oloo_RUN4.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.h2oloo_RUN4), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/h2oloo_RUN4.pdf) 

- :material-rename: **Run ID:** h2oloo_RUN4 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `f123fca5fb65b3560e63a83ee5aee3c0` 
- :material-text: **Run description:** This is our automatic baseline method for which we train T5-base to conduct query reformulation by concatenating historical raw queries on CANARD dataset. As for retrieval, we combine BERT embedding search and BM25 as first-stage retrieval and use T5 3B for the second stage reranker. 

---
#### HBKU_t2_1v1 
[**`Results`**](./results.md#hbku_t2_1v1), [**`Participants`**](./participants.md#hbku), [**`Proceedings`**](./proceedings.md#hbku-at-trec-2020-conversational-multi-stage-retrieval-with-pseudo-relevance-feedback), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HBKU_t2_1v1.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HBKU_t2_1v1), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HBKU_t2_1v1.pdf) 

- :material-rename: **Run ID:** HBKU_t2_1v1 
- :fontawesome-solid-user-group: **Participant:** HBKU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `7a305d6c9e2a798026ea88e7c7404f14` 
- :material-text: **Run description:** The methods used for this submission aims to resolve context in two phases. The first phase is to incorporate historical query context by extracting keywords from previous queries in a method based on last year's winning team's algorithm. The second phase incorporates context from top-k retrieved passages through extracting top-k keywords from these passages using TF-IDF. After that, a pre-trained BERT passage re-ranker was used. This run adds context from both historical queries and top-3 keywords from top-3 retrieved passages of the current query. After that, the list was re-ranked using a pre-trained BERT re-ranker trained on MS Marco dataset.  Both historical query and passage context was used in the re-ranking phase. 

---
#### HBKU_t2_1v1_mnl 
[**`Results`**](./results.md#hbku_t2_1v1_mnl), [**`Participants`**](./participants.md#hbku), [**`Proceedings`**](./proceedings.md#hbku-at-trec-2020-conversational-multi-stage-retrieval-with-pseudo-relevance-feedback), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HBKU_t2_1v1_mnl.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HBKU_t2_1v1_mnl), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HBKU_t2_1v1_mnl.pdf) 

- :material-rename: **Run ID:** HBKU_t2_1v1_mnl 
- :fontawesome-solid-user-group: **Participant:** HBKU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `77b29cf1edd0bf5a7ad6c03b67c5ba5b` 
- :material-text: **Run description:** The methods used for this submission aims to resolve context in two phases. The first phase is to incorporate historical query context by extracting keywords from previous queries in a method based on last year's winning team's algorithm. The second phase incorporates context from top-k retrieved passages through extracting top-k keywords from these passages using TF-IDF. After that, a pre-trained BERT passage re-ranker was used. This run adds context from both historical queries and top-3 keywords from top-3 retrieved passages of the current query. After that, the list was re-ranked using a pre-trained BERT re-ranker trained on MS Marco dataset.  Both historical query and passage context was used in the re-ranking phase. 

---
#### HBKU_t2_1v2 
[**`Results`**](./results.md#hbku_t2_1v2), [**`Participants`**](./participants.md#hbku), [**`Proceedings`**](./proceedings.md#hbku-at-trec-2020-conversational-multi-stage-retrieval-with-pseudo-relevance-feedback), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HBKU_t2_1v2.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HBKU_t2_1v2), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HBKU_t2_1v2.pdf) 

- :material-rename: **Run ID:** HBKU_t2_1v2 
- :fontawesome-solid-user-group: **Participant:** HBKU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `81b135a0340dd5c97bded66b6ed2f756` 
- :material-text: **Run description:** The methods used for this submission aims to resolve context in two phases. The first phase is to incorporate historical query context by extracting keywords from previous queries in a method based on last year's winning team's algorithm. The second phase incorporates context from top-k retrieved passages through extracting top-k keywords from these passages using TF-IDF. After that, a pre-trained BERT passage re-ranker was used. This run adds context from both historical queries and top-3 keywords from top-3 retrieved passages of the current query. After that, the list was re-ranked using a pre-trained BERT re-ranker trained on MS Marco dataset.  Only historical query context was used in the re-ranking phase. 

---
#### HBKU_t2_1v2_mnl 
[**`Results`**](./results.md#hbku_t2_1v2_mnl), [**`Participants`**](./participants.md#hbku), [**`Proceedings`**](./proceedings.md#hbku-at-trec-2020-conversational-multi-stage-retrieval-with-pseudo-relevance-feedback), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HBKU_t2_1v2_mnl.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HBKU_t2_1v2_mnl), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HBKU_t2_1v2_mnl.pdf) 

- :material-rename: **Run ID:** HBKU_t2_1v2_mnl 
- :fontawesome-solid-user-group: **Participant:** HBKU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `7f71df0913be15fd31e37ef4963fad35` 
- :material-text: **Run description:** The methods used for this submission aims to resolve context in two phases. The first phase is to incorporate historical query context by extracting keywords from previous queries in a method based on last year's winning algorithm. The second phase incorporates context from top-k retrieved passages through extracting top-k keywords from these passages using TF-IDF. After that, a pre-trained BERT passage re-ranker was used. This run adds context from both historical queries and top-3 keywords from top-3 retrieved passages of the current query. After that, the list was re-ranked using a pre-trained BERT re-ranker trained on MS Marco dataset.  Only historical query context was used in the re-ranking phase. 

---
#### HBKU_t5_1v1 
[**`Results`**](./results.md#hbku_t5_1v1), [**`Participants`**](./participants.md#hbku), [**`Proceedings`**](./proceedings.md#hbku-at-trec-2020-conversational-multi-stage-retrieval-with-pseudo-relevance-feedback), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HBKU_t5_1v1.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HBKU_t5_1v1), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HBKU_t5_1v1.pdf) 

- :material-rename: **Run ID:** HBKU_t5_1v1 
- :fontawesome-solid-user-group: **Participant:** HBKU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `a0fa22f2157b4210188a73caa6cda5cf` 
- :material-text: **Run description:** The methods used for this submission aims to resolve context in two phases. The first phase is to incorporate historical query context by extracting keywords from previous queries in a method based on last year's winning team's algorithm. The second phase incorporates context from top-k retrieved passages through extracting top-k keywords from these passages using TF-IDF. After that, a pre-trained BERT passage re-ranker was used. This run first categorizes queries into two categories: explicit and implicit. Explicit queries are ones that contain no pronouns while implicit queries contain at least one pronoun. Explicit queries are assumed to contain sufficient information and so only historical query context is used to expand the query. Implicit queries contain at least one pronoun and are assumed to need more clarification, so both historical queries and passage context is added. After that, the list was re-ranked using a pre-trained BERT re-ranker trained on MS Marco dataset. Both historical query and passage context was used in the re-ranking phase. 

---
#### HBKU_t5_1v1_mnl 
[**`Results`**](./results.md#hbku_t5_1v1_mnl), [**`Participants`**](./participants.md#hbku), [**`Proceedings`**](./proceedings.md#hbku-at-trec-2020-conversational-multi-stage-retrieval-with-pseudo-relevance-feedback), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HBKU_t5_1v1_mnl.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HBKU_t5_1v1_mnl), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HBKU_t5_1v1_mnl.pdf) 

- :material-rename: **Run ID:** HBKU_t5_1v1_mnl 
- :fontawesome-solid-user-group: **Participant:** HBKU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `a50d2fc3c0a91b8bce862190048fb002` 
- :material-text: **Run description:** The methods used for this submission aims to resolve context in two phases. The first phase is to incorporate historical query context by extracting keywords from previous queries in a method based on last year's winning team's algorithm. The second phase incorporates context from top-k retrieved passages through extracting top-k keywords from these passages using TF-IDF. After that, a pre-trained BERT passage re-ranker was used. This run first categorizes queries into two categories: explicit and implicit. Explicit queries are ones that contain no pronouns while implicit queries contain at least one pronoun. Explicit queries are assumed to contain sufficient information and so only historical query context is used to expand the query. Implicit queries contain at least one pronoun and are assumed to need more clarification, so both historical queries and passage context is added. After that, the list was re-ranked using a pre-trained BERT re-ranker trained on MS Marco dataset. Both historical query and passage context was used in the re-ranking phase. 

---
#### HBKU_t5_1v2 
[**`Results`**](./results.md#hbku_t5_1v2), [**`Participants`**](./participants.md#hbku), [**`Proceedings`**](./proceedings.md#hbku-at-trec-2020-conversational-multi-stage-retrieval-with-pseudo-relevance-feedback), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HBKU_t5_1v2.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HBKU_t5_1v2), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HBKU_t5_1v2.pdf) 

- :material-rename: **Run ID:** HBKU_t5_1v2 
- :fontawesome-solid-user-group: **Participant:** HBKU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `b9cd235bf4248a7d79eefc24fa3cdcc1` 
- :material-text: **Run description:** The methods used for this submission aims to resolve context in two phases. The first phase is to incorporate historical query context by extracting keywords from previous queries in a method based on last year's winning team's algorithm. The second phase incorporates context from top-k retrieved passages through extracting top-k keywords from these passages using TF-IDF. After that, a pre-trained BERT passage re-ranker was used. This run first categorizes queries into two categories: explicit and implicit. Explicit queries are ones that contain no pronouns while implicit queries contain at least one pronoun. Explicit queries are assumed to contain sufficient information and so only historical query context is used to expand the query. Implicit queries contain at least one pronoun and are assumed to need more clarification, so both historical queries and passage context is added. After that, the list was re-ranked using a pre-trained BERT re-ranker trained on MS Marco dataset. only historical query context was used in the re-ranking phase.  

---
#### HBKU_t5_1v2_mnl 
[**`Results`**](./results.md#hbku_t5_1v2_mnl), [**`Participants`**](./participants.md#hbku), [**`Proceedings`**](./proceedings.md#hbku-at-trec-2020-conversational-multi-stage-retrieval-with-pseudo-relevance-feedback), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HBKU_t5_1v2_mnl.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HBKU_t5_1v2_mnl), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HBKU_t5_1v2_mnl.pdf) 

- :material-rename: **Run ID:** HBKU_t5_1v2_mnl 
- :fontawesome-solid-user-group: **Participant:** HBKU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `6287c98c6838f8d7bde98673220d90d1` 
- :material-text: **Run description:** The methods used for this submission aims to resolve context in two phases. The first phase is to incorporate historical query context by extracting keywords from previous queries in a method based on last year's winning team's algorithm. The second phase incorporates context from top-k retrieved passages through extracting top-k keywords from these passages using TF-IDF. After that, a pre-trained BERT passage re-ranker was used. This run first categorizes queries into two categories: explicit and implicit. Explicit queries are ones that contain no pronouns while implicit queries contain at least one pronoun. Explicit queries are assumed to contain sufficient information and so only historical query context is used to expand the query. Implicit queries contain at least one pronoun and are assumed to need more clarification, so both historical queries and passage context is added. After that, the list was re-ranked using a pre-trained BERT re-ranker trained on MS Marco dataset. Only historical query context was used in the re-ranking phase. 

---
#### hist_attention 
[**`Results`**](./results.md#hist_attention), [**`Participants`**](./participants.md#usi), [**`Proceedings`**](./proceedings.md#extending-the-use-of-previous-relevant-utterances-for-response-ranking-in-conversational-search), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.hist_attention.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.hist_attention), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/hist_attention.pdf) 

- :material-rename: **Run ID:** hist_attention 
- :fontawesome-solid-user-group: **Participant:** USI 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** canonical 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `ed112a2cdd88361ca88ee36308d0c812` 
- :material-text: **Run description:** Conversational query understanding is done by expanding the current query with previous relevant queries, which are predicted by a model trained on CAsTUR dataset. Initial ranking is done by Anserini's QL (default params), followed by a re-ranking model that encodes the queries and passages with Albert, pretrained on MS MARCO passage ranking task. The encodings of the current utterance are combined with previous relevant ones with self-attentive sum component. The summed representation is then forwarded through a classifier to predict passage relevance. 

---
#### hist_concat 
[**`Results`**](./results.md#hist_concat), [**`Participants`**](./participants.md#usi), [**`Proceedings`**](./proceedings.md#extending-the-use-of-previous-relevant-utterances-for-response-ranking-in-conversational-search), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.hist_concat.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.hist_concat), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/hist_concat.pdf) 

- :material-rename: **Run ID:** hist_concat 
- :fontawesome-solid-user-group: **Participant:** USI 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** canonical 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `4031b9094e0ccfc0316dd09f504b3663` 
- :material-text: **Run description:** Conversational query understanding is done by expanding the current query with previous relevant queries, which are predicted by a model trained on CAsTUR dataset. Initial ranking is done by Anserini's QL (default params), followed by a re-ranking model that encodes the queries and passages with Albert, pretrained on MS MARCO passage ranking task. The encodings of the current utterance are concatenated with the most relevant query and the corresponding automatic canonical response from history. The representation is then forwarded through a classifier to predict passage relevance. 

---
#### HPCLab-CNR-run1 
[**`Results`**](./results.md#hpclab-cnr-run1), [**`Participants`**](./participants.md#hpclab-cnr), [**`Proceedings`**](./proceedings.md#topical-enrichment-of-conversational-search-utterances-participation-of-the-hpclab-cnr-team-in-cast-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HPCLab-CNR-run1.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HPCLab-CNR-run1), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HPCLab-CNR-run1.pdf) 

- :material-rename: **Run ID:** HPCLab-CNR-run1 
- :fontawesome-solid-user-group: **Participant:** HPCLab-CNR 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** canonical 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `42a865f51addf95cdf38db73aaac924a` 
- :material-text: **Run description:** For this run, we manually labeled the utterances as SE, FT, PT, PR (labels are described in the external resources).  If the utterance label is SE, no rewriting is applied. If the utterance label is FT, the automatic approach rewrites the utterance by using the topic extracted from the first utterance of the conversation. When the label is PT, the rewriting is performed using the topic extracted from the previous utterance in the conversation.    The topics are extracted from utterances using the noun chunks (objects or subjects). When the utterance label is PR the keywords of the automatic canonical response (after stopwords removal) of the previous utterance are added.  Every utterance is rewritten using the automatically-rewritten utterance of the previous turn so that the context is propagated through the conversation. INDRI is used for indexing and querying the CAST-2020 dataset. In particular, we index the two datasets (MS-MARCO and TREC CAR) by removing stopwords and using the Krovetz stemmer. As a querying method, we use the INDRI language model based on Dirichlet smoothing with parameter \mu = 2500. We also apply pseudo-relevance feedback (PRF) based on the RM3 algorithm. The query enrichment is done with 20 keywords taken from the top-20 results with mixing parameter \gamma = 0.5. Lastly, the passage re-ranking is performed using a model that fine-tunes the BERT base pre-trained model for re-ranking on the MS-MARCO passage retrieval dataset. INDRI retrieves 1000 results and all of them are used for the re-ranking step.  

---
#### HPCLab-CNR-run2 
[**`Results`**](./results.md#hpclab-cnr-run2), [**`Participants`**](./participants.md#hpclab-cnr), [**`Proceedings`**](./proceedings.md#topical-enrichment-of-conversational-search-utterances-participation-of-the-hpclab-cnr-team-in-cast-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HPCLab-CNR-run2.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HPCLab-CNR-run2), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HPCLab-CNR-run2.pdf) 

- :material-rename: **Run ID:** HPCLab-CNR-run2 
- :fontawesome-solid-user-group: **Participant:** HPCLab-CNR 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `36c6c79fc2e25e566e22f86dfb31c5d1` 
- :material-text: **Run description:** This run uses raw utterances. The approach automatically rewrites the utterance by using the topics extracted from all previous utterances of the conversation. The topics are extracted from utterances using the noun chunks (objects or subjects). We use a list of cue phrases to detect a topic shift, when a topic shift occurs the list of topics to propagate is updated. INDRI is used for indexing and querying the CAST-2020 dataset. In particular, we index the two datasets (MS-MARCO and TREC CAR) by removing stopwords and using the Krovetz stemmer. As a querying method, we use the INDRI language model based on Dirichlet smoothing with parameter \mu = 2500. We also apply pseudo-relevance feedback (PRF) based on the RM3 algorithm. The query enrichment is done with 20 keywords taken from the top-20 results with mixing parameter \gamma = 0.5. Lastly, the passage re-ranking is performed using a model that fine-tunes the BERT base pre-trained model for re-ranking on the MS-MARCO passage retrieval dataset. INDRI retrieves 1000 results and all of them are used for the re-ranking step. 

---
#### HPCLab-CNR-run3 
[**`Results`**](./results.md#hpclab-cnr-run3), [**`Participants`**](./participants.md#hpclab-cnr), [**`Proceedings`**](./proceedings.md#topical-enrichment-of-conversational-search-utterances-participation-of-the-hpclab-cnr-team-in-cast-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HPCLab-CNR-run3.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HPCLab-CNR-run3), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HPCLab-CNR-run3.pdf) 

- :material-rename: **Run ID:** HPCLab-CNR-run3 
- :fontawesome-solid-user-group: **Participant:** HPCLab-CNR 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** canonical 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `e2d305d05c0eb4254bea125a274d5cc8` 
- :material-text: **Run description:** This run uses the raw utterances and the candidate responses for the automatic utterances provided by CAST. It leverages utterance manual labels (i.e., SE, FT, PT, PR) which are described in the external resources. No rewriting is applied if the label is SE. When the label is FT, the utterance rewriting is preformed with the topic extracted from the first utterance, while when it is PT the topic from the previous utterance is used. For utterances labeled as PR, the topic extracted from the automatic canonical response of the previous utterance is added, too. The topics are extracted from utterances using the noun chunks (objects or subjects in the utterances) that are recognized as named entities by TagMe (with threshold = 0.1). The same approach is applied to extract the topic from the automatic canonical responses. Every utterance is rewritten using the automatically-rewritten utterance of the previous turn so that the context is propagated through the conversation. INDRI is used for indexing and querying the CAST-2020 dataset. In particular, we index the two datasets (MS-MARCO and TREC CAR) by removing stopwords and using the Krovetz stemmer. As a querying method, we use the INDRI language model based on Dirichlet smoothing with parameter \mu = 2500. We also apply pseudo-relevance feedback (PRF) based on the RM3 algorithm. The query enrichment is done with 20 keywords taken from the top-20 results with mixing parameter \gamma = 0.5. Lastly, the passage re-ranking is performed using a model that fine-tunes the BERT base pre-trained model for re-ranking on the MS-MARCO passage retrieval dataset. INDRI retrieves 1000 results and all of them are used for the re-ranking step. 

---
#### HPCLab-CNR-run4 
[**`Results`**](./results.md#hpclab-cnr-run4), [**`Participants`**](./participants.md#hpclab-cnr), [**`Proceedings`**](./proceedings.md#topical-enrichment-of-conversational-search-utterances-participation-of-the-hpclab-cnr-team-in-cast-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.HPCLab-CNR-run4.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.HPCLab-CNR-run4), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/HPCLab-CNR-run4.pdf) 

- :material-rename: **Run ID:** HPCLab-CNR-run4 
- :fontawesome-solid-user-group: **Participant:** HPCLab-CNR 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `8e347219320cebcb93c1563f6a60fd37` 
- :material-text: **Run description:** This run uses raw utterances. The approach automatically rewrites the utterance by using the current and previous topics. The topics are extracted from utterances using the noun chunks (objects or subjects) and full verbs. We use a list of cue phrases to detect a topic shift. At the beginning of the conversation, the current topic is the topic extracted from the first utterance, when a topic shift is detected the current topic is updated.  INDRI is used for indexing and querying the CAST-2020 dataset. In particular, we index the two datasets (MS-MARCO and TREC CAR) by removing stopwords and using the Krovetz stemmer. As a querying method, we use the INDRI language model based on Dirichlet smoothing with parameter \mu = 2500. We also apply pseudo-relevance feedback (PRF) based on the RM3 algorithm. The query enrichment is done with 20 keywords taken from the top-20 results with mixing parameter \gamma = 0.5. Lastly, the passage re-ranking is performed using a model that fine-tunes the BERT base pre-trained model for re-ranking on the MS-MARCO passage retrieval dataset. INDRI retrieves 1000 results and all of them are used for the re-ranking step. 

---
#### humanQR 
[**`Results`**](./results.md#humanqr), [**`Participants`**](./participants.md#uvailps), [**`Proceedings`**](./proceedings.md#leveraging-query-resolution-and-reading-comprehension-for-conversational-passage-retrieval), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.humanQR.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.humanQR), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/humanQR.pdf) 

- :material-rename: **Run ID:** humanQR 
- :fontawesome-solid-user-group: **Participant:** UvA.ILPS 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `f31fe6172bee0e176645f4f6cb831a7c` 
- :material-text: **Run description:** PR BM25 Anserini + BERT for passage ranking combined with the score from Roberta fine-tuned on MRQA formulated as a binary classification task (paragraph contains answer or doesn't) 

---
#### ielab-bertAQ 
[**`Results`**](./results.md#ielab-bertaq), [**`Participants`**](./participants.md#ielab), [**`Proceedings`**](./proceedings.md#ielab-for-trec-conversational-assistance-track-cast-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.ielab-bertAQ.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.ielab-bertAQ), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/ielab-bertAQ.pdf) 

- :material-rename: **Run ID:** ielab-bertAQ 
- :fontawesome-solid-user-group: **Participant:** ielab 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `4bdd2477f76ae881f9b90b70b0e8f9a0` 
- :material-text: **Run description:** BERT based re-ranking of BM25+RM3 baseline. Top k=10 documents only are re-ranked using BERT. For each topic, for utterances beyond the first, the first query is appended. 

---
#### ielab-bertPRFAQ 
[**`Results`**](./results.md#ielab-bertprfaq), [**`Participants`**](./participants.md#ielab), [**`Proceedings`**](./proceedings.md#ielab-for-trec-conversational-assistance-track-cast-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.ielab-bertPRFAQ.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.ielab-bertPRFAQ), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/ielab-bertPRFAQ.pdf) 

- :material-rename: **Run ID:** ielab-bertPRFAQ 
- :fontawesome-solid-user-group: **Participant:** ielab 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `df4cb673c3b367b24ba8d3f60135193e` 
- :material-text: **Run description:** BERT based re-ranking of BM25+RM3 baseline. Top k=10 documents only are re-ranked using BERT. When re-ranking, the query and the first p=3 passages are embedded using BERT to create the embedding for the query. For each topic, for utterances beyond the first, the first query is appended. 

---
#### ielab-bm25AQ 
[**`Results`**](./results.md#ielab-bm25aq), [**`Participants`**](./participants.md#ielab), [**`Proceedings`**](./proceedings.md#ielab-for-trec-conversational-assistance-track-cast-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.ielab-bm25AQ.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.ielab-bm25AQ), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/ielab-bm25AQ.pdf) 

- :material-rename: **Run ID:** ielab-bm25AQ 
- :fontawesome-solid-user-group: **Participant:** ielab 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `bc22231e86fe09d17bf2943ba6a2ba04` 
- :material-text: **Run description:** BM25+RM3 baseline. For each topic, for utterances beyond the first, the first query is appended. 

---
#### ielab-bm25T5QLM 
[**`Results`**](./results.md#ielab-bm25t5qlm), [**`Participants`**](./participants.md#ielab), [**`Proceedings`**](./proceedings.md#ielab-for-trec-conversational-assistance-track-cast-2020), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.ielab-bm25T5QLM.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.ielab-bm25T5QLM), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/ielab-bm25T5QLM.pdf) 

- :material-rename: **Run ID:** ielab-bm25T5QLM 
- :fontawesome-solid-user-group: **Participant:** ielab 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `5eea924c92da0a84c094fb10c337e9cd` 
- :material-text: **Run description:** Generative QLM based on T5 from initial baseline retrieval using BM25+RM3.  For each topic, for utterances beyond the first, the first query is appended. 

---
#### man_polyu1 
[**`Results`**](./results.md#man_polyu1), [**`Participants`**](./participants.md#polyu_some), [**`Proceedings`**](./proceedings.md#polyu-at-trec-2020-conversational-assistant-track-query-reformulation-with-heuristic-topic-phrases-discovery), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.man_polyu1.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.man_polyu1), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/man_polyu1.pdf) 

- :material-rename: **Run ID:** man_polyu1 
- :fontawesome-solid-user-group: **Participant:** POLYU_SOME 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `22879358a9c05a84e8063a0dae562fd6` 
- :material-text: **Run description:** BM25+BERT rerank 

---
#### PAS_BQUERY_MER 
[**`Results`**](./results.md#pas_bquery_mer), [**`Participants`**](./participants.md#cmu-lti), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.PAS_BQUERY_MER.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.PAS_BQUERY_MER), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/PAS_BQUERY_MER.pdf) 

- :material-rename: **Run ID:** PAS_BQUERY_MER 
- :fontawesome-solid-user-group: **Participant:** CMU-LTI 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `487e2867c09bb661c4211d89fb7c93dd` 
- :material-text: **Run description:** For a given query, a term selection classifier first selects important terms from the conversation history relevant to the given query. The query along with the selected terms is used to retrieve 30 passages. From these passages, 3 phrases are selected (using off-the-shelf coreference resolution) as additional contextual terms. The terms selected by the classifier and the 3 phrases and the initial query itself is used for first-round of passage retrieval which is done using Indri. Finally, BERT (fine-tuned on MS-MARCO) is used for reranking the top 500 initially retrieved passages. The input to BERT are the passages, the original query, and terms selected by the classifier and the top 3 phrases selected from passages. 

---
#### PAS_RQUERY_MER 
[**`Results`**](./results.md#pas_rquery_mer), [**`Participants`**](./participants.md#cmu-lti), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.PAS_RQUERY_MER.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.PAS_RQUERY_MER), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/PAS_RQUERY_MER.pdf) 

- :material-rename: **Run ID:** PAS_RQUERY_MER 
- :fontawesome-solid-user-group: **Participant:** CMU-LTI 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `ff05cfe310a3663c3e5fbc83b2e8c8bf` 
- :material-text: **Run description:** For a given query, a POS tagger first selects important terms (adjectives, nouns, verbs) from the conversation history relevant to the given query. The query along with the selected terms is used to retrieve 30 passages. From these passages, 3 phrases are selected (using off-the-shelf coreference resolution) as additional contextual terms. The terms selected by the classifier and the 3 phrases and the initial query itself is used for first-round of passage retrieval which is done using Indri. Finally, BERT (fine-tuned on MS-MARCO) is used for reranking. The input to BERT are the passages, the original query, and terms selected using the POS tagger and the top 3 phrases selected from passages. 

---
#### PASO_BQUERO_MER 
[**`Results`**](./results.md#paso_bquero_mer), [**`Participants`**](./participants.md#cmu-lti), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.PASO_BQUERO_MER.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.PASO_BQUERO_MER), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/PASO_BQUERO_MER.pdf) 

- :material-rename: **Run ID:** PASO_BQUERO_MER 
- :fontawesome-solid-user-group: **Participant:** CMU-LTI 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `236c8581f077a2e04736b5431af628d5` 
- :material-text: **Run description:** Two different initial retrievals are done for a given query. First, a term selection classifier is used for selecting important terms for the conversation history. The selected terms along with the input query is used for retrieving a set of passages. In the next step, the query is used to retrieve 30 passages. From these passages, 3 phrases are selected (using off-the-shelf coreference resolution) as additional contextual terms. Both the two initial retrievals are reranked using BERT (fine-tuned on MS-MARCO). The two reranked lists are then merged into one. 

---
#### quretecNoRerank 
[**`Results`**](./results.md#quretecnorerank), [**`Participants`**](./participants.md#uvailps), [**`Proceedings`**](./proceedings.md#leveraging-query-resolution-and-reading-comprehension-for-conversational-passage-retrieval), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.quretecNoRerank.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.quretecNoRerank), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/quretecNoRerank.pdf) 

- :material-rename: **Run ID:** quretecNoRerank 
- :fontawesome-solid-user-group: **Participant:** UvA.ILPS 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** canonical 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `c2f9b6b2dc1b62f7321a9c71c59c0e8c` 
- :material-text: **Run description:** PR: BM25 Anserini QR input: current turn query, the answer from the previous turn only taken from the automatic canonical answer and the previous queries 

---
#### quretecQR 
[**`Results`**](./results.md#quretecqr), [**`Participants`**](./participants.md#uvailps), [**`Proceedings`**](./proceedings.md#leveraging-query-resolution-and-reading-comprehension-for-conversational-passage-retrieval), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.quretecQR.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.quretecQR), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/quretecQR.pdf) 

- :material-rename: **Run ID:** quretecQR 
- :fontawesome-solid-user-group: **Participant:** UvA.ILPS 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** canonical 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `c3414182c1f9a79cbeb57a3eae0aec3e` 
- :material-text: **Run description:** PR: BM25 Anserini + BERT for passage ranking combined with the score from Roberta fine-tuned on MRQA formulated as a binary classification task (paragraph contains answer or doesn't) QR input: current turn query, the answer from the previous turn only taken from the automatic canonical answer and the previous queries 

---
#### raw_polyu1 
[**`Results`**](./results.md#raw_polyu1), [**`Participants`**](./participants.md#polyu_some), [**`Proceedings`**](./proceedings.md#polyu-at-trec-2020-conversational-assistant-track-query-reformulation-with-heuristic-topic-phrases-discovery), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.raw_polyu1.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.raw_polyu1), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/raw_polyu1.pdf) 

- :material-rename: **Run ID:** raw_polyu1 
- :fontawesome-solid-user-group: **Participant:** POLYU_SOME 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `ea9d5f36fb2f6a8512677bea10f345d1` 
- :material-text: **Run description:** query rewriting + BM25 +Bert rerank 

---
#### rewrite_albert 
[**`Results`**](./results.md#rewrite_albert), [**`Participants`**](./participants.md#usi), [**`Proceedings`**](./proceedings.md#extending-the-use-of-previous-relevant-utterances-for-response-ranking-in-conversational-search), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.rewrite_albert.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.rewrite_albert), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/rewrite_albert.pdf) 

- :material-rename: **Run ID:** rewrite_albert 
- :fontawesome-solid-user-group: **Participant:** USI 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `0de3ed7093ff261f9aa0aeebe8b2dacd` 
- :material-text: **Run description:** Initial retrieval is performed with Anserini QL with Dirichlet smoothing with automatically rewritten queries provided by TREC organizers. Re-ranking step utilizes Albert, which is pretrained on MS MARCO passage re-ranking task. 

---
#### T5_BERT100 
[**`Results`**](./results.md#t5_bert100), [**`Participants`**](./participants.md#nova-search), [**`Proceedings`**](./proceedings.md#nova-at-trec-2020-conversational-assistance-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.T5_BERT100.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.T5_BERT100), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/T5_BERT100.pdf) 

- :material-rename: **Run ID:** T5_BERT100 
- :fontawesome-solid-user-group: **Participant:** nova-search 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `d32f02aada2a30c797e0906c7643d295` 
- :material-text: **Run description:** To maintain the conversational context across turns we finetuned a T5 model on the conversational query rewriting task using the CANARD dataset. The input to the T5 model is the concatenation of the current query and all of the previous queries, and the output of the model is the query rewritten for that turn. Initial retrieval is done with the rewritten query using a QL model with LMD smoothing. After this we perform reranking using a BERT model finetuned on the MS MARCO dataset for the passage reranking task. 

---
#### umass_4prev_rm3 
[**`Results`**](./results.md#umass_4prev_rm3), [**`Participants`**](./participants.md#umass_ciir), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.umass_4prev_rm3.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.umass_4prev_rm3), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/umass_4prev_rm3.pdf) 

- :material-rename: **Run ID:** umass_4prev_rm3 
- :fontawesome-solid-user-group: **Participant:** UMass_CIIR 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `45392c98b8d4cf0f6711156cabe22932` 
- :material-text: **Run description:** In this run, we expand the current turn of each topic using the expansion terms that are extrated from the feedback documnets of 4 previous turns. We use the automatic runs as the feedback documents.  

---
#### umass_curr_rm3 
[**`Results`**](./results.md#umass_curr_rm3), [**`Participants`**](./participants.md#umass_ciir), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.umass_curr_rm3.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.umass_curr_rm3), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/umass_curr_rm3.pdf) 

- :material-rename: **Run ID:** umass_curr_rm3 
- :fontawesome-solid-user-group: **Participant:** UMass_CIIR 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `8dc18ea29db29ac493d4d86d0afa7644` 
- :material-text: **Run description:** In this run we expand the current turn of the conversation with the terms from top retrieved documents for that turn. We use the provided automatic baseline as the top documents to extract expansion terms.  

---
#### WatACBase 
[**`Results`**](./results.md#watacbase), [**`Participants`**](./participants.md#waterlooclarke), [**`Proceedings`**](./proceedings.md#waterlooclarke-at-the-trec-2020-conversational-assistant-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.WatACBase.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.WatACBase), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/WatACBase.pdf) 

- :material-rename: **Run ID:** WatACBase 
- :fontawesome-solid-user-group: **Participant:** WaterlooClarke 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `50c1a4e0bf57288ad51470a4afce4fd7` 
- :material-text: **Run description:** Utterance augmented with the initial utterance of the conversation, with coreference resolution.  

---
#### WatACBaseRe 
[**`Results`**](./results.md#watacbasere), [**`Participants`**](./participants.md#waterlooclarke), [**`Proceedings`**](./proceedings.md#waterlooclarke-at-the-trec-2020-conversational-assistant-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.WatACBaseRe.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.WatACBaseRe), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/WatACBaseRe.pdf) 

- :material-rename: **Run ID:** WatACBaseRe 
- :fontawesome-solid-user-group: **Participant:** WaterlooClarke 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `d6d96811d78e59a6a128f776c2225913` 
- :material-text: **Run description:** Utterance augmented with the initial utterance of the conversation, with coreference resolution. Reranked with Bert Passage reranking.   

---
#### WatACGPT2Re 
[**`Results`**](./results.md#watacgpt2re), [**`Participants`**](./participants.md#waterlooclarke), [**`Proceedings`**](./proceedings.md#waterlooclarke-at-the-trec-2020-conversational-assistant-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.WatACGPT2Re.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.WatACGPT2Re), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/WatACGPT2Re.pdf) 

- :material-rename: **Run ID:** WatACGPT2Re 
- :fontawesome-solid-user-group: **Participant:** WaterlooClarke 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `1771f61b6b94c8611e2ca7f6485ce1b4` 
- :material-text: **Run description:** automatically rewritten utterances, reranked with Bert Passage reranker 

---
#### WatACReAll 
[**`Results`**](./results.md#watacreall), [**`Participants`**](./participants.md#waterlooclarke), [**`Proceedings`**](./proceedings.md#waterlooclarke-at-the-trec-2020-conversational-assistant-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.WatACReAll.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.WatACReAll), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/WatACReAll.pdf) 

- :material-rename: **Run ID:** WatACReAll 
- :fontawesome-solid-user-group: **Participant:** WaterlooClarke 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/20/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `04399d0045e30b66502daa9e4482c8b6` 
- :material-text: **Run description:** Reranking the retrieved documents by the following runs: WatACBase, WatACBaseRe and WatACGPT2Re 

---
#### WLU_ManUttOnly 
[**`Results`**](./results.md#wlu_manuttonly), [**`Participants`**](./participants.md#wlu), [**`Proceedings`**](./proceedings.md#wilfrid-laurier-university-at-the-nist-trec-2020-conversational-assistance-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.WLU_ManUttOnly.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.WLU_ManUttOnly), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/WLU_ManUttOnly.pdf) 

- :material-rename: **Run ID:** WLU_ManUttOnly 
- :fontawesome-solid-user-group: **Participant:** WLU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `e88f3d3c9a123aa3b038ab6d63842e17` 
- :material-text: **Run description:** This run was generated by encoding the queries and documents using a pre-trained BERT 768 model. This data was used by our deep learning model (trained on CAST & MARCO data) to rerank documents. 

---
#### WLU_RawUttOnly 
[**`Results`**](./results.md#wlu_rawuttonly), [**`Participants`**](./participants.md#wlu), [**`Proceedings`**](./proceedings.md#wilfrid-laurier-university-at-the-nist-trec-2020-conversational-assistance-track), [**`Input`**](https://trec.nist.gov/results/trec29/cast/input.WLU_RawUttOnly.gz), [**`Summary`**](https://trec.nist.gov/results/trec29/cast/summary.treceval.WLU_RawUttOnly), [**`Appendix`**](https://trec.nist.gov/pubs/trec29/appendices/cast/WLU_RawUttOnly.pdf) 

- :material-rename: **Run ID:** WLU_RawUttOnly 
- :fontawesome-solid-user-group: **Participant:** WLU 
- :material-format-text: **Track:** Conversational Assistance 
- :material-calendar: **Year:** 2020 
- :material-upload: **Submission:** 8/19/2020 
- :fontawesome-solid-user-gear: **Type:** raw 
- :material-text-search: **Task:** primary 
- :material-fingerprint: **MD5:** `fdbc364cb5f1545b810ad4b700b80a62` 
- :material-text: **Run description:** This run was generated by encoding the queries and documents using a pre-trained BERT 768 model. This data was used by our deep learning model (trained on CAST & MARCO data) to rerank documents 

---
