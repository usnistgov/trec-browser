# Runs - Fair Ranking 2022 

#### 0mt5 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.0mt5.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.0mt5.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/0mt5.pdf) 

- :material-rename: **Run ID:** 0mt5 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `d8947118a8b0a04ac55287c1b42704a0` 
- :material-text: **Run description:** monoT5-3B-10K 

---
#### 0mt5_e 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.0mt5_e.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.0mt5_e.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/0mt5_e.pdf) 

- :material-rename: **Run ID:** 0mt5_e 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `b94c89733266ee6d964c0d94c68011f0` 
- :material-text: **Run description:** monoT5-3B-10K 

---
#### 0mt5_p 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.0mt5_p.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.0mt5_p.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/0mt5_p.pdf) 

- :material-rename: **Run ID:** 0mt5_p 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `eb4e674e2b459874d0a8d1b17bea8dd1` 
- :material-text: **Run description:** monoT5-3B-10K with post-processing 

---
#### 0mt5_p_e 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.0mt5_p_e.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.0mt5_p_e.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/0mt5_p_e.pdf) 

- :material-rename: **Run ID:** 0mt5_p_e 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `01de573380030f8282737bc53642196c` 
- :material-text: **Run description:** monoT5-3B-10K with post-processing 

---
#### ans_bm25 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.ans_bm25.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.ans_bm25.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/ans_bm25.pdf) 

- :material-rename: **Run ID:** ans_bm25 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `bfab18f1673e56980533277acfcab8b8` 
- :material-text: **Run description:** Anserini/Pyserini BM25 

---
#### ans_bm25_e 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.ans_bm25_e.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.ans_bm25_e.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/ans_bm25_e.pdf) 

- :material-rename: **Run ID:** ans_bm25_e 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `8110d8958536037ab180e0f23330583a` 
- :material-text: **Run description:** Anserini/Pyserini BM25 

---
#### bm25_p 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.bm25_p.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.bm25_p.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/bm25_p.pdf) 

- :material-rename: **Run ID:** bm25_p 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `8576a3cffb2659e0ff355108de681db1` 
- :material-text: **Run description:** Anserini/Pyserini BM25 with post-processing 

---
#### bm25_p_e 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.bm25_p_e.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.bm25_p_e.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/bm25_p_e.pdf) 

- :material-rename: **Run ID:** bm25_p_e 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `dfdf8d5f55067e7b2602b46aa5b248f7` 
- :material-text: **Run description:** Anserini/Pyserini BM25 with post-processing 

---
#### FRT_attention 
[**`Participants`**](./participants.md#v-ryerson), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.FRT_attention.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.FRT_attention.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/FRT_attention.pdf) 

- :material-rename: **Run ID:** FRT_attention 
- :fontawesome-solid-user-group: **Participant:** V-Ryerson 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/3/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `c3e8a598aed4172fda89f09899440c3e` 
- :material-text: **Run description:** This run uses fairness categories including article quality. We extracted 'article-text', 'hyperlinks', 'categories' from text corpus to build BM25 corpus.   We selected top 1000 relevant wikipages based on these BM25 scores and semantic score using BERT embeddings.  We re-rank these 1000 wikipages based on categories distribution, we compute fairness categories using top 5000 relevant wikipages with BM25 and BERT scores We used the inverse distribution (1-category distirbution) as weight, and re-ranked wikipages such that minority groups would get more attentions.  

---
#### FRT_constraint 
[**`Participants`**](./participants.md#v-ryerson), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.FRT_constraint.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.FRT_constraint.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/FRT_constraint.pdf) 

- :material-rename: **Run ID:** FRT_constraint 
- :fontawesome-solid-user-group: **Participant:** V-Ryerson 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/3/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `cbb4e5344e9c3fe91c329424792219bf` 
- :material-text: **Run description:** This run uses fairness categories including article quality. We extracted 'article-text', 'hyperlinks', 'categories' from text corpus to build BM25 corpus.   We selected top 1000 relevant wikipages based on these BM25 scores and semantic score using BERT embeddings.  We re-rank these 1000 wikipages based on categories distribution, we compute fairness categories using top 5000 relevant wikipages with BM25 and BERT scores We used the difference between categories distribution and ranked list's categories distribution as weight, and re-ranked wikipages such that category distirbution is enforced at each rank.  

---
#### FRT_diversity 
[**`Participants`**](./participants.md#v-ryerson), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.FRT_diversity.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.FRT_diversity.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/FRT_diversity.pdf) 

- :material-rename: **Run ID:** FRT_diversity 
- :fontawesome-solid-user-group: **Participant:** V-Ryerson 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/3/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `fdaeda6ac64e564ec858a13b05541307` 
- :material-text: **Run description:** This run uses fairness categories including article quality. We extracted 'article-text', 'hyperlinks', 'categories' from text corpus to build BM25 corpus.   We selected top 1000 relevant wikipages based on these BM25 scores and semantic score using BERT embeddings.  We re-rank these 1000 wikipages based on categories diversity, by converting fairness categories into vector and compute the distances.  

---
#### rmit_cidda_ir_1 
[**`Participants`**](./participants.md#rmit_cidda_ir), [**`Proceedings`**](./proceedings.md#rmit-cidda-ir-at-the-trec-2022-fair-ranking-track), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.rmit_cidda_ir_1.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.rmit_cidda_ir_1.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/rmit_cidda_ir_1.pdf) 

- :material-rename: **Run ID:** rmit_cidda_ir_1 
- :fontawesome-solid-user-group: **Participant:** rmit_cidda_ir 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `87ffbfde8a895b7f0f13aba1f7f15ebf` 
- :material-text: **Run description:** Ad hoc retrieval using BM25 as provided by Pyserini with default parameters (k1=0.9, b=0.4) over a text-only collection. 

---
#### rmit_cidda_ir_2 
[**`Participants`**](./participants.md#rmit_cidda_ir), [**`Proceedings`**](./proceedings.md#rmit-cidda-ir-at-the-trec-2022-fair-ranking-track), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.rmit_cidda_ir_2.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.rmit_cidda_ir_2.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/rmit_cidda_ir_2.pdf) 

- :material-rename: **Run ID:** rmit_cidda_ir_2 
- :fontawesome-solid-user-group: **Participant:** rmit_cidda_ir 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `2b9252d4aceec415c839b47ef43d4151` 
- :material-text: **Run description:** Ad hoc retrieval using BM25 as provided by Pyserini with default parameters (k1=0.9, b=0.4) over a text-only collection; re-ranking using explicit search result diversification using PM-2 with two attributes (num_sitelinks and category); ranking fusion using Reciprocal Ranking Fusion (RRF) to obtain the final ranking. 

---
#### rmit_cidda_ir_3 
[**`Participants`**](./participants.md#rmit_cidda_ir), [**`Proceedings`**](./proceedings.md#rmit-cidda-ir-at-the-trec-2022-fair-ranking-track), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.rmit_cidda_ir_3.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.rmit_cidda_ir_3.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/rmit_cidda_ir_3.pdf) 

- :material-rename: **Run ID:** rmit_cidda_ir_3 
- :fontawesome-solid-user-group: **Participant:** rmit_cidda_ir 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `3e5bc8e423512b856fc18b152e9c7b03` 
- :material-text: **Run description:** Ad hoc retrieval using BM25 as provided by Pyserini with default parameters (k1=0.9, b=0.4) over a text-only collection;  re-ranking using explicit search result diversification using PM-2 with all attributes; ranking fusion with an adapted version of Reciprocal Ranking Fusion (RRF) that uses a weighting schema to obtain the final rank. The weighting schema is obtained using heuristics in an Analytical Hierarchical Process (AHP). 

---
#### rmit_cidda_ir_4 
[**`Participants`**](./participants.md#rmit_cidda_ir), [**`Proceedings`**](./proceedings.md#rmit-cidda-ir-at-the-trec-2022-fair-ranking-track), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.rmit_cidda_ir_4.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.rmit_cidda_ir_4.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/rmit_cidda_ir_4.pdf) 

- :material-rename: **Run ID:** rmit_cidda_ir_4 
- :fontawesome-solid-user-group: **Participant:** rmit_cidda_ir 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `da3c7fd94fba75353662e5fe354224ea` 
- :material-text: **Run description:** Ad hoc retrieval using BM25 as provided by Pyserini with default parameters (k1=0.9, b=0.4) over a text-only collection;  re-ranking using explicit search result diversification using PM-2 with two attributes (years and category); ranking fusion using Reciprocal Ranking Fusion (RRF) to obtain the final ranking. 

---
#### rmit_cidda_ir_5 
[**`Participants`**](./participants.md#rmit_cidda_ir), [**`Proceedings`**](./proceedings.md#rmit-cidda-ir-at-the-trec-2022-fair-ranking-track), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.rmit_cidda_ir_5.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.rmit_cidda_ir_5.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/rmit_cidda_ir_5.pdf) 

- :material-rename: **Run ID:** rmit_cidda_ir_5 
- :fontawesome-solid-user-group: **Participant:** rmit_cidda_ir 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `1e8e8c03720f39b4e9c012b33cc70650` 
- :material-text: **Run description:** Ad hoc retrieval using BM25 as provided by Pyserini with default parameters (k1=0.9, b=0.4) over a text-only collection;  re-ranking using explicit search result diversification using PM-2 with two attributes (gender and category); ranking fusion using Reciprocal Ranking Fusion (RRF) to obtain the final ranking. 

---
#### rmit_cidda_ir_6 
[**`Participants`**](./participants.md#rmit_cidda_ir), [**`Proceedings`**](./proceedings.md#rmit-cidda-ir-at-the-trec-2022-fair-ranking-track), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.rmit_cidda_ir_6.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.rmit_cidda_ir_6.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/rmit_cidda_ir_6.pdf) 

- :material-rename: **Run ID:** rmit_cidda_ir_6 
- :fontawesome-solid-user-group: **Participant:** rmit_cidda_ir 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `62b20b6506506223a45845b8cbd54440` 
- :material-text: **Run description:** Ad hoc retrieval using BM25 as provided by Pyserini with default parameters (k1=0.9, b=0.4) over a text-only collection;  re-ranking using explicit search result diversification using PM-2 with all attributes; ranking fusion with an adapted version of Reciprocal Ranking Fusion (RRF) that uses a weighting schema to obtain the final rank. The weighting schema is obtained using heuristics in an Analytical Hierarchical Process (AHP). 

---
#### rmit_cidda_ir_7 
[**`Participants`**](./participants.md#rmit_cidda_ir), [**`Proceedings`**](./proceedings.md#rmit-cidda-ir-at-the-trec-2022-fair-ranking-track), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.rmit_cidda_ir_7.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.rmit_cidda_ir_7.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/rmit_cidda_ir_7.pdf) 

- :material-rename: **Run ID:** rmit_cidda_ir_7 
- :fontawesome-solid-user-group: **Participant:** rmit_cidda_ir 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `4f981ee76f5c05e85c36765228f01afe` 
- :material-text: **Run description:** Ad hoc retrieval using BM25 as provided by Pyserini with default parameters (k1=0.9, b=0.4) over a text-only collection;  re-ranking using explicit search result diversification using PM-2 with all attributes; ranking fusion with an adapted version of Reciprocal Ranking Fusion (RRF) that uses a weighting schema to obtain the final rank. The weighting schema is obtained using heuristics in an Analytical Hierarchical Process (AHP). 

---
#### rmit_cidda_ir_8 
[**`Participants`**](./participants.md#rmit_cidda_ir), [**`Proceedings`**](./proceedings.md#rmit-cidda-ir-at-the-trec-2022-fair-ranking-track), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.rmit_cidda_ir_8.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.rmit_cidda_ir_8.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/rmit_cidda_ir_8.pdf) 

- :material-rename: **Run ID:** rmit_cidda_ir_8 
- :fontawesome-solid-user-group: **Participant:** rmit_cidda_ir 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `2ac0e675f38f17a1a02ab95e421daa5f` 
- :material-text: **Run description:** Ad hoc retrieval using BM25 as provided by Pyserini with default parameters (k1=0.9, b=0.4) over a text-only collection;  re-ranking using explicit search result diversification using PM-2 with all attributes; ranking fusion with an adapted version of Reciprocal Ranking Fusion (RRF) that uses a weighting schema to obtain the final rank. The weighting schema is obtained using heuristics in an Analytical Hierarchical Process (AHP). 

---
#### tmt5 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.tmt5.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.tmt5.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/tmt5.pdf) 

- :material-rename: **Run ID:** tmt5 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `6bb6c4986c29c050635def1abbe328a7` 
- :material-text: **Run description:** monoT5-3B-10K trained for 5K steps on TREC-F21+22 

---
#### tmt5_e 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.tmt5_e.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.tmt5_e.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/tmt5_e.pdf) 

- :material-rename: **Run ID:** tmt5_e 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `46477be95c38ca13b3d91f587498f825` 
- :material-text: **Run description:** monoT5-3B-10K trained for 5K steps on TREC-F21+22 

---
#### tmt5_p 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.tmt5_p.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.tmt5_p.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/tmt5_p.pdf) 

- :material-rename: **Run ID:** tmt5_p 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `34742b41b8b10f870f022ecffba4e105` 
- :material-text: **Run description:** monoT5-3B-10K trained for 5K steps on TREC-F21+22 with post-processing 

---
#### tmt5_p_e 
[**`Participants`**](./participants.md#h2oloo), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.tmt5_p_e.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.tmt5_p_e.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/tmt5_p_e.pdf) 

- :material-rename: **Run ID:** tmt5_p_e 
- :fontawesome-solid-user-group: **Participant:** h2oloo 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/6/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `472a85502db833818aac7d457c934be5` 
- :material-text: **Run description:** monoT5-3B-10K trained for 5K steps on TREC-F21+22 with post-processing 

---
#### UDInfo_F_bm25 
[**`Participants`**](./participants.md#udel_fang), [**`Proceedings`**](./proceedings.md#an-exploration-of-learning-to-re-rank-using-a-two-step-framework-for-fair-ranking), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UDInfo_F_bm25.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UDInfo_F_bm25.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UDInfo_F_bm25.pdf) 

- :material-rename: **Run ID:** UDInfo_F_bm25 
- :fontawesome-solid-user-group: **Participant:** udel_fang 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `a22c2319fd23a6869bc9f4aa5ee9fa46` 
- :material-text: **Run description:** This is a BM25 base run before doing any fairness-aware re-rank. The rank is based on non-increasing bm25 scores (keywords-doc) only. 

---
#### UDInfo_F_lgbm2 
[**`Participants`**](./participants.md#udel_fang), [**`Proceedings`**](./proceedings.md#an-exploration-of-learning-to-re-rank-using-a-two-step-framework-for-fair-ranking), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UDInfo_F_lgbm2.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UDInfo_F_lgbm2.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UDInfo_F_lgbm2.pdf) 

- :material-rename: **Run ID:** UDInfo_F_lgbm2 
- :fontawesome-solid-user-group: **Participant:** udel_fang 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `11e415880599fd7d2b9cacf9e0c1b8f7` 
- :material-text: **Run description:** This is a static method without quality scores in ranking.  We use Sentence-BERT to embed documents, queries, and fairness annotations (e.g., for gender, the fairness annotations are the sentence "male female non-binary"). Then, we compute similarity scores between these embeddings as our contextual features for training a GBDT-based model. We encode fairness by modifying the ground truth label. The ground truth we used was a weighted sum of both relevance and point-wise AWRF using log decay. Last, for this specific run, we use the trained GBDT-based model to re-rank every 20 documents in the BM25 base run. 

---
#### UDInfo_F_lgbm4 
[**`Participants`**](./participants.md#udel_fang), [**`Proceedings`**](./proceedings.md#an-exploration-of-learning-to-re-rank-using-a-two-step-framework-for-fair-ranking), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UDInfo_F_lgbm4.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UDInfo_F_lgbm4.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UDInfo_F_lgbm4.pdf) 

- :material-rename: **Run ID:** UDInfo_F_lgbm4 
- :fontawesome-solid-user-group: **Participant:** udel_fang 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `244861c7eb00dfa8798fadd5962b100f` 
- :material-text: **Run description:** This is a static method without quality scores in ranking. We use Sentence-BERT to embed documents, queries, and fairness annotations (e.g., for gender, the fairness annotations are the sentence "male female non-binary"). Then, we compute similarity scores between these embeddings as our contextual features for training a GBDT-based model. We encode fairness by modifying the ground truth label. The ground truth we used was a weighted sum of both relevance and point-wise AWRF using log decay. Last, for this specific run, we use the trained GBDT-based model to re-rank every 40 documents in the BM25 base run. 

---
#### UDInfo_F_mlp2 
[**`Participants`**](./participants.md#udel_fang), [**`Proceedings`**](./proceedings.md#an-exploration-of-learning-to-re-rank-using-a-two-step-framework-for-fair-ranking), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UDInfo_F_mlp2.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UDInfo_F_mlp2.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UDInfo_F_mlp2.pdf) 

- :material-rename: **Run ID:** UDInfo_F_mlp2 
- :fontawesome-solid-user-group: **Participant:** udel_fang 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `0f9187d959a0c9335c98e0d11e3dcb1d` 
- :material-text: **Run description:** This is a static method without quality scores in ranking. We use Sentence-BERT to embed documents, queries, and fairness annotations (e.g., for gender, the fairness annotations are the sentence "male female non-binary"). Then, we compute similarity scores between these embeddings as our contextual features for training an MLP model. We encode fairness by modifying the ground truth label. The ground truth we used was a weighted sum of both relevance and point-wise AWRF using log decay. Last, for this specific run, we use the trained MLP model to re-rank every 20 documents in the BM25 base run. 

---
#### UDInfo_F_mlp4 
[**`Participants`**](./participants.md#udel_fang), [**`Proceedings`**](./proceedings.md#an-exploration-of-learning-to-re-rank-using-a-two-step-framework-for-fair-ranking), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UDInfo_F_mlp4.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UDInfo_F_mlp4.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UDInfo_F_mlp4.pdf) 

- :material-rename: **Run ID:** UDInfo_F_mlp4 
- :fontawesome-solid-user-group: **Participant:** udel_fang 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `4f6688c2ab7e32b9983bd0265e87eb02` 
- :material-text: **Run description:** This is a static method without quality scores in ranking. We use Sentence-BERT to embed documents, queries, and fairness annotations (e.g., for gender, the fairness annotations are the sentence "male female non-binary"). Then, we compute similarity scores between these embeddings as our contextual features for training an MLP model. We encode fairness by modifying the ground truth label. The ground truth we used was a weighted sum of both relevance and point-wise AWRF using log decay. Last, for this specific run, we use the trained MLP model to re-rank every 40 documents in the BM25 base run. 

---
#### UoGRelvOnlyT1 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGRelvOnlyT1.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGRelvOnlyT1.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGRelvOnlyT1.pdf) 

- :material-rename: **Run ID:** UoGRelvOnlyT1 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `da57e057a2cf6d2ab6fcd1bbd84cd643` 
- :material-text: **Run description:** A relevance-only baseline with no fairness intervention for Task 1. The retrieval was done with ColBERT. 

---
#### UogTRelvOnlyT2 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UogTRelvOnlyT2.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UogTRelvOnlyT2.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UogTRelvOnlyT2.pdf) 

- :material-rename: **Run ID:** UogTRelvOnlyT2 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/2/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `9f67fabaa4cee02a933be43418dc2cac` 
- :material-text: **Run description:** A sequence of the same ranking for every query based only on relevance. There are no fairness interventions in this run. The retrieval was done with ColBERT. This run will be used as a baseline. 

---
#### UoGTrExpE1 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGTrExpE1.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGTrExpE1.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGTrExpE1.pdf) 

- :material-rename: **Run ID:** UoGTrExpE1 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `d5f601422fc433cc1ac79fc11f58ca46` 
- :material-text: **Run description:** UoGTrExpE1 uses a heuristic approach to rerank an initial retrieval set using expected exposure targets. The reranking uses adapted techniques from diversification, such as PM-2. The target exposures in these runs dont follow the metric but try to evenly distribute the exposure between attributes.  

---
#### UoGTrExpE2 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGTrExpE2.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGTrExpE2.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGTrExpE2.pdf) 

- :material-rename: **Run ID:** UoGTrExpE2 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `e57a17af1589e2309946ec63dfe523bd` 
- :material-text: **Run description:** Similar to UoGTrExpE1 this run uses a heuristic approach to rerank an initial retrieval set using expected exposure targets. In the run traditional diversification techniques are adapted. The initial retrieval was done with ColBERT. The expected exposure targets were partly created with the help of the released metric and the relevance estimation of our initial retrieval.  

---
#### UoGTrMabSAED 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGTrMabSAED.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGTrMabSAED.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGTrMabSAED.pdf) 

- :material-rename: **Run ID:** UoGTrMabSAED 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `7f8a32d921cfd6d44e7cb4412b254c34` 
- :material-text: **Run description:** UoGTrMabSAED uses a Multi-Armed Bandit approach. An agent tries to find the optimal strategy when adding rankings to the sequence. The rankings are selected from a pool of rankings where each ranking is optimised to be fair to only one individual attribute. This approach uses a variation of an epsilon-decay strategy. No weights are used in the creation of the pool of the rankings. 

---
#### UoGTrMabSaNR 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGTrMabSaNR.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGTrMabSaNR.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGTrMabSaNR.pdf) 

- :material-rename: **Run ID:** UoGTrMabSaNR 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `f85fa340632d8e359e26d3ad51ebf9bc` 
- :material-text: **Run description:** UoGTrMabSA uses a Multi-Armed Bandit approach. An agent tries to find the optimal strategy when adding rankings to the sequence. The rankings are selected from a pool of rankings where each ranking is optimised to be fair to only one individual attribute. The initial retrieval was done with ColBERT as implemented in PyTerrier. In contrast to our other MAB approaches, there is no randomisation in this approach and no weighting of the rankings is used.  

---
#### UoGTrMabSaWR 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGTrMabSaWR.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGTrMabSaWR.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGTrMabSaWR.pdf) 

- :material-rename: **Run ID:** UoGTrMabSaWR 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `cfc46e537e859938805b4304f33a3b74` 
- :material-text: **Run description:** UoGTrMabSaWR uses a Multi-Armed Bandit approach. An agent tries to find the optimal strategy when adding rankings to the sequence. The rankings are selected from a pool of rankings where each ranking is optimised to be fair to only one individual attribute. The initial retrieval was done with ColBERT as implemented in PyTerrier. This run uses no weighting and a randomisation approach for the exploring phase of the agent.   

---
#### UoGTrMabWeSA 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGTrMabWeSA.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGTrMabWeSA.editors.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGTrMabWeSA.pdf) 

- :material-rename: **Run ID:** UoGTrMabWeSA 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** editors 
- :material-fingerprint: **MD5:** `87f581979c12d3286d813e88099d3f3d` 
- :material-text: **Run description:** UoGTrMabWeSA uses a Multi-Armed Bandit approach. An agent tries to find the optimal strategy when adding rankings to the sequence. The rankings are selected from a pool of rankings. For every protected attribute, there are 3 different rankings with different fairness-relevance relationships. The initial retrieval was done with PyTerrier-Colbert 

---
#### UoGTrQE 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGTrQE.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGTrQE.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGTrQE.pdf) 

- :material-rename: **Run ID:** UoGTrQE 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `616d268b48730f98f4f73a8782fc0d08` 
- :material-text: **Run description:** UoGTrQE uses a traditional query expansion method to expand a query with the goal to improve the distribution of documents in a fair manner.  

---
#### UoGTrT1ColPRF 
[**`Participants`**](./participants.md#uogtr), [**`Input`**](https://trec.nist.gov/results/trec31/fair/input.UoGTrT1ColPRF.gz), [**`Summary`**](https://trec.nist.gov/results/trec31/fair/summary.UoGTrT1ColPRF.coord.tsv), [**`Appendix`**](https://trec.nist.gov/pubs/trec31/appendices/fair/UoGTrT1ColPRF.pdf) 

- :material-rename: **Run ID:** UoGTrT1ColPRF 
- :fontawesome-solid-user-group: **Participant:** UoGTr 
- :material-format-text: **Track:** Fair Ranking 
- :material-calendar: **Year:** 2022 
- :material-upload: **Submission:** 9/1/2022 
- :material-text-search: **Task:** coordinators 
- :material-fingerprint: **MD5:** `b80f0c255910894927924b57bed06df0` 
- :material-text: **Run description:** UoGTrT1ColPRF is an adaption of ColbertPRF to the fairness task.  

---
