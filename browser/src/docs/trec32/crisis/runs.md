# Runs - CrisisFACTs 2023 

#### drdqn-all 
[**`Participants`**](./participants.md#darthreca) 

- :material-rename: **Run ID:** drdqn-all 
- :fontawesome-solid-user-group: **Participant:** DarthReca 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/29/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `4a30cf93d6a34e4e3f47f19fe7c52ed8` 
- :material-text: **Run description:** The system makes use of a DQN for text retrieval, topic modeling (BERTopic) for the selected relevant texts, and an abstractive summarizer for each generated cluster (BART-large-CNN). The importance is a normalized weighted mean of the difference between the Q-values of the retrieval system and the length of the text list that supports the fact. The system weights differently the news from the social networks. 

---
#### drdqn-notopic 
[**`Participants`**](./participants.md#darthreca) 

- :material-rename: **Run ID:** drdqn-notopic 
- :fontawesome-solid-user-group: **Participant:** DarthReca 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/30/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `c87a1f87207ba93f8eb7dd65a4307175` 
- :material-text: **Run description:** The retrieval system is based on a DQN and for each of the retrieved text an abstractive summarizer (BART-large-CNN) is applied.  The importance is calculated as normalized weighted mean difference of Q-Values of the retrieval system and the type of source. 

---
#### Human_Info_Lab-FM-A 
[**`Participants`**](./participants.md#human_info_lab) 

- :material-rename: **Run ID:** Human_Info_Lab-FM-A 
- :fontawesome-solid-user-group: **Participant:** Human_Info_Lab 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/2/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `3600804d62a7fb6140231aa97eecdb6b` 
- :material-text: **Run description:** In this system, the indicative terms provided in the user profile are extended by using the Keybert library. Then, the streams are filtered by using the extended indicative terms. After that, for each stream a set of facts is generated by using a clause-based approach to open information extraction called ClausIE (FM-A). Then, the generated facts are filtered again by using extended indicative terms. For assigning the importance to each fact, we calculate the closeness centrality of each fact in a graph which is generated based on the similarity of facts to each other and the similarity of facts to the extended indicative terms and queries from user profiles. Finally, the importance scores are scaled to [0,1] and the duplicates are dropped. For assigning the importance to each fact, we calculate the closeness centrality of each fact in a graph which is generated based on the similarity of facts to each other and the similarity of facts to the extended indicative terms and queries from user profiles. Finally, the importance scores are scaled to [0,1]. 

---
#### Human_Info_Lab-FM-B 
[**`Participants`**](./participants.md#human_info_lab) 

- :material-rename: **Run ID:** Human_Info_Lab-FM-B 
- :fontawesome-solid-user-group: **Participant:** Human_Info_Lab 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/2/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `9a06072f64db28545c0eba003a13e059` 
- :material-text: **Run description:** In this system, the indicative terms provided in the user profile are extended by using the Keybert library. Then, the streams are filtered by using the extended indicative terms. After that, for each stream a set of facts are generated by using Constituency Parsing with a Self-Attentive Encoder (FM-B). Then, the generated facts are filtered again by using extended indicative terms. For assigning the importance to each fact, we calculate the closeness centrality of each fact in a graph which is generated based on the similarity of facts to each other and the similarity of facts to the extended indicative terms and queries from user profiles. Finally, the importance scores are scaled to [0,1] and the duplicates are dropped. For assigning the importance to each fact, we calculate the closeness centrality of each fact in a graph which is generated based on the similarity of facts to each other and the similarity of facts to the extended indicative terms and queries from user profiles. Then, the importance scores are scaled to [0,1]. 

---
#### IDACCS_GPT3.5 
[**`Participants`**](./participants.md#idaccs) 

- :material-rename: **Run ID:** IDACCS_GPT3.5 
- :fontawesome-solid-user-group: **Participant:** IDACCS 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/31/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `da22cb97452a819a48d68469152af79a` 
- :material-text: **Run description:** We used GPT3.5 to generate a summary, then segmented and found the best matching factText for attribution The importance score for the ith sentence is  s_i = (n- i)/n in the summary 

---
#### IDACCS_occams_extract 
[**`Participants`**](./participants.md#idaccs) 

- :material-rename: **Run ID:** IDACCS_occams_extract 
- :fontawesome-solid-user-group: **Participant:** IDACCS 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/30/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `45b0e5d11557c2829d7e6e0e1dd66e64` 
- :material-text: **Run description:** occams is an extractive summarization system that approximately solves the bounded maximal coverage problem.  We used bigrams with the LOG_COUNTS term weighting scheme. We use an extractive summarizer occams to select the most representative sentences from the pyterrier run. The importance score for the ith sentence is  s_i = (n- i)/n  

---
#### IDACCS_occamsHybridGPT3.5 
[**`Participants`**](./participants.md#idaccs) 

- :material-rename: **Run ID:** IDACCS_occamsHybridGPT3.5 
- :fontawesome-solid-user-group: **Participant:** IDACCS 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/31/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `764ac96d9e3bed89e391cde22c1bfb10` 
- :material-text: **Run description:** We use a hybrid approach which generates an extractive summary via occams and then uses GPT3.5 to generate a summary, which is a paraphrase of the occams extract. The importance score for the ith sentence is  s_i = (n- i)/n in the summary 

---
#### ilp_mmr 
[**`Participants`**](./participants.md#ohm) 

- :material-rename: **Run ID:** ilp_mmr 
- :fontawesome-solid-user-group: **Participant:** OHM 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/28/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `f9c89814685c92ad212c33409bb8cb44` 
- :material-text: **Run description:** The system consists of three successive components: 1: Lexical retrieval with BM25 (+ BO1 query expansion) based on indicative terms + query text (top 250 p. query) 2: Re-ranking with monoT5-large based on query text (top 50 p. query) 3: ILP-system for diversified sentence selection with respect to covered entities and MMR for re-ranking (top 150 - 200 stream items) The importance for each stream-item is calculated with MMR by the formula: lambda x score_relevance - (1-lambda) x score_redundancy score_relevance: sum of all document-query pair scores score_redundancy: max(tf-idf cosine similarity of stream-items already added) 

---
#### IRLabIITBHU_BM25_1 
[**`Participants`**](./participants.md#irlab_iit_bhu) 

- :material-rename: **Run ID:** IRLabIITBHU_BM25_1 
- :fontawesome-solid-user-group: **Participant:** IRLAB_IIT_BHU 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/31/2023 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-fingerprint: **MD5:** `9a35ff8be3f089e2a4e574022ff8b921` 
- :material-text: **Run description:** We are calculating importance using BM25. It uses enhances TF-IDF (Term Frequency-Inverse Document Frequency) model. We are calculating importance using BM25. Tt uses enhances TF-IDF (Term Frequency-Inverse Document Frequency) model. We are finalizing importance by dividing respective importance by max of importance. 

---
#### IRLabIITBHU_DFReeKLIM_1 
[**`Participants`**](./participants.md#irlab_iit_bhu) 

- :material-rename: **Run ID:** IRLabIITBHU_DFReeKLIM_1 
- :fontawesome-solid-user-group: **Participant:** IRLAB_IIT_BHU 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/1/2023 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-fingerprint: **MD5:** `1e76a46f3e86800c9e7fe294a85bf20d` 
- :material-text: **Run description:** I'm using DFReeKLIM model. The Divergence from Randomness (DFR) models in information retrieval aim to estimate the importance of a term or a combination of terms in a document with respect to a query.  I'm using DFReeKLIM model. The Divergence from Randomness (DFR) models in information retrieval aim to estimate the importance of a term or a combination of terms in a document with respect to a query. These models calculate a relevance score based on the statistical properties of the terms in the document and the query. I'm are finalizing importance by dividing respective importance by max of importance 

---
#### IRLabIITBHU_DFReeKLIM_2 
[**`Participants`**](./participants.md#irlab_iit_bhu) 

- :material-rename: **Run ID:** IRLabIITBHU_DFReeKLIM_2 
- :fontawesome-solid-user-group: **Participant:** IRLAB_IIT_BHU 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/1/2023 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-fingerprint: **MD5:** `e1e43ae63bf17c091753907912769653` 
- :material-text: **Run description:** I'm using DFReeKLIM model. The Divergence from Randomness (DFR) models in information retrieval aim to estimate the importance of a term or a combination of terms in a document with respect to a query. I'm using DFReeKLIM model. The Divergence from Randomness (DFR) models in information retrieval aim to estimate the importance of a term or a combination of terms in a document with respect to a query. These models calculate a relevance score based on the statistical properties of the terms in the document and the query. I'm are finalizing importance by dividing respective importance by max of importance 

---
#### llama 
[**`Participants`**](./participants.md#umd_hcil) 

- :material-rename: **Run ID:** llama 
- :fontawesome-solid-user-group: **Participant:** umd_hcil 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/30/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `3cacd7daaaada168dace8e6df37766ab` 
- :material-text: **Run description:** This method uses a standard retrieval model to get a list of relevant sentences for each query. Then, for that query, we have a prompt that asks a transformer model to summarize the list of facts, ranked by their relevance to the query. This step produces a one-to-three-sentence summary for each query on each event-day pair. We then aggregate all the queries for a given event-day into a single document that we then ask GPT-3.5 to rewrite into a summary of the most critical content. We then the importance of facts for this event-day pair based on overlap with this summary. We ask ChatGPT to rewrite a list of facts into a summary that includes only the most critical information. We then score importance of our facts based on overlap with this summary of critical content and rank by this important score. 

---
#### llama_13b_chat 
[**`Participants`**](./participants.md#ohm) 

- :material-rename: **Run ID:** llama_13b_chat 
- :fontawesome-solid-user-group: **Participant:** OHM 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/1/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `3c0228530c40b91a9022563cd9ede162` 
- :material-text: **Run description:** The system consists of three successive components: 1: Lexical retrieval with BM25 (+ BO1 query expansion) based on indicative terms + query text (top 250 p. query) 2: Re-ranking with monoT5-large based on query text (top 50 p. query) 3: LLaMA-2 (13B chat model) extract and summarizes facts with respect to the query (top 10 p. query) The importance for each stream-item is calculated by the mean of score_relevance from all source documents used for generating the abstractive fact. score_relevance: re-ranking score for document-query pair 

---
#### nm-gpt35 
[**`Participants`**](./participants.md#nm) 

- :material-rename: **Run ID:** nm-gpt35 
- :fontawesome-solid-user-group: **Participant:** NM 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/30/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `4f51b15ced305acc0d150e891f683b8e` 
- :material-text: **Run description:** We employ a two-step pipeline for constructing abstractive facts from social media and online news. The first step is a retrieval step that uses the pre-defined user queries to search for relevant documents. Then, we use the top-k documents to compose a prompt that is submitted to a large language model (LLM). The second step of our pipeline consists of using the LLM to summarize the most important facts given the top-k documents. This pipeline is executed for each event-day pair. In this run, we use BM25+monoT5 in the retrieval step and GPT-3.5-turbo-16k in the LLM reasoning step. We use the top 30 documents from the retrieval step. We use a search strategy based on BM25 and monoT5 (a neural reranker). We use the scores assigned by the search strategy to each document (e.g., a tweet) and use as importance the mean score of the documents that were used as a basis for the fact.  

---
#### nm-gpt35-bm25 
[**`Participants`**](./participants.md#nm) 

- :material-rename: **Run ID:** nm-gpt35-bm25 
- :fontawesome-solid-user-group: **Participant:** NM 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/2/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `09adc3751bb07d986a8d442482769fe6` 
- :material-text: **Run description:** We employ a two-step pipeline for constructing abstractive facts from social media and online news. The first step is a retrieval step that uses the pre-defined user queries to search for relevant documents. Then, we use the top-k documents to compose a prompt that is submitted to a large language model (LLM). The second step of our pipeline consists of using the LLM to summarize the most important facts given the top-k documents. This pipeline is executed for each event-day pair. In this run, we use BM25 in the retrieval step and GPT-3.5-turbo-16k in the LLM reasoning step. We use the top 10 documents from the retrieval step.  We use a search strategy based on BM25 and monoT5 (a neural reranker). We use the scores assigned by the search strategy to each document (e.g., a tweet) and use as importance the mean score of the documents that were used as a basis for the fact.  

---
#### nm-gpt4 
[**`Participants`**](./participants.md#nm) 

- :material-rename: **Run ID:** nm-gpt4 
- :fontawesome-solid-user-group: **Participant:** NM 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/1/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `ddace1b8dae98e67a1b88fbb27d93e69` 
- :material-text: **Run description:** We employ a two-step pipeline for constructing abstractive facts from social media and online news. The first step is a retrieval step that uses the pre-defined user queries to search for relevant documents. Then, we use the top-k documents to compose a prompt that is submitted to a large language model (LLM). The second step of our pipeline consists of using the LLM to summarize the most important facts given the top-k documents. This pipeline is executed for each event-day pair. In this run, we use BM25+monoT5 in the retrieval step and GPT-4-8k in the LLM reasoning step. We use the top 10 documents from the retrieval step.  We use a search strategy based on BM25 and monoT5 (a neural reranker). We use the scores assigned by the search strategy to each document (e.g., a tweet) and use as importance the mean score of the documents that were used as a basis for the fact.  

---
#### nut-kslab01 
[**`Participants`**](./participants.md#nut-kslab) 

- :material-rename: **Run ID:** nut-kslab01 
- :fontawesome-solid-user-group: **Participant:** nut-kslab 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/31/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `826783be3eb090672bc9b54581a0ef93` 
- :material-text: **Run description:** leverages the BM25 model to process the CrisisFACTS dataset, identify relevant facts using queries, compute importance metrics raw importance counts and sums, and then transforms them into normalized values 

---
#### Siena.Baseline1 
[**`Participants`**](./participants.md#sienaclteam) 

- :material-rename: **Run ID:** Siena.Baseline1 
- :fontawesome-solid-user-group: **Participant:** SienaCLTeam 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/29/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `96e07c0b0ab11ce9ee3e01aa078c44fe` 
- :material-text: **Run description:** This is a baseline run generated from the baseline script present in the CrisisFACTS GitHub repository. The system calculates importance based on the number of matches each fact makes. It uses the same system as baseline 1 in the baseline script. 

---
#### Siena.FactTrigrams1 
[**`Participants`**](./participants.md#sienaclteam) 

- :material-rename: **Run ID:** Siena.FactTrigrams1 
- :fontawesome-solid-user-group: **Participant:** SienaCLTeam 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/29/2023 
- :fontawesome-solid-user-gear: **Type:** manual 
- :material-fingerprint: **MD5:** `8b66011c46d6a8b519d673f5984a8e21` 
- :material-text: **Run description:** The system uses the facts to gather a large set of trigrams. The trigrams are then scored against the queries to see which trigrams perform best. These trigrams are then added to the queries they scored well against and the baseline script is run using these expanded queries. The system calculates importance based on the number of matches each fact makes. It uses the same system as baseline 1 in the baseline script. 

---
#### Siena.WikiTrigrams1 
[**`Participants`**](./participants.md#sienaclteam) 

- :material-rename: **Run ID:** Siena.WikiTrigrams1 
- :fontawesome-solid-user-group: **Participant:** SienaCLTeam 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/29/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `77b9de1d542999dd3dcef4ca671f8945` 
- :material-text: **Run description:** The system uses the wikipedia page associated with each event to gather a large set of trigrams. The trigrams are then scored against the queries to see which trigrams perform best. These trigrams are then added to the queries they scored well against and the baseline script is run using these expanded queries. The system calculates importance based on the number of matches each fact makes. It uses the same system as baseline 1 in the baseline script. 

---
#### Siena.WikiTrigrams2 
[**`Participants`**](./participants.md#sienaclteam) 

- :material-rename: **Run ID:** Siena.WikiTrigrams2 
- :fontawesome-solid-user-group: **Participant:** SienaCLTeam 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 8/29/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `e0796e545a46f596aa99fd6003df2fd0` 
- :material-text: **Run description:** The system uses the wikipedia page associated with each event to gather a large set of trigrams. The trigrams are then scored against the queries to see which trigrams perform best. These trigrams are then added to the queries they scored well against and the baseline script is run using these expanded queries. The system calculates importance based on the sum of the scores for each fact match. It uses the same system as baseline 2 in the baseline script. 

---
#### TorontoMU_Word2Vec_TFIDF 
[**`Participants`**](./participants.md#v-torontomu) 

- :material-rename: **Run ID:** TorontoMU_Word2Vec_TFIDF 
- :fontawesome-solid-user-group: **Participant:** V-TorontoMU 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/1/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `6d7beb2fb50de25dc1ad9b1e03f91af2` 
- :material-text: **Run description:** To produce the run, the code employs TF-IDF for vector representation based on word importance in documents and Word2Vec to derive context-aware embeddings; a combination of these approaches through weighted summation yields the final document rankings. The code employs a hybrid strategy that combines the discrete, frequency-based importance of words in the document space using TF-IDF, with the continuous, semantic space representation using Word2Vec. By weighting these vectors, the system gauges both the lexical prominence and contextual meaning of terms in the documents for assessing relevance. 

---
#### V-TorontoMU-DFReeKLIM 
[**`Participants`**](./participants.md#v-torontomu) 

- :material-rename: **Run ID:** V-TorontoMU-DFReeKLIM 
- :fontawesome-solid-user-group: **Participant:** V-TorontoMU 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/1/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `d41d8cd98f00b204e9800998ecf8427e` 
- :material-text: **Run description:** The code leverages the pyTerrier library to perform information retrieval using the DFReeKLIM weighting model, indexing preprocessed documents from specific crisis datasets. The queries, derived from user profiles, are then processed and matched against the indexed corpus to retrieve and rank relevant documents based on the significance of terms in the document collection relative to their occurrence in the query.  The code doesn't directly calculate "importance" in a traditional sense. Instead, it uses the DFReeKLIM weighting model within the pyTerrier library to rank documents for a given query, which internally evaluates the significance of terms in the document collection relative to their occurrence in the query, thereby providing an implicit measure of document importance for a specific query. 

---
#### V-TorontoMU-DFReeKLIM-v2 
[**`Participants`**](./participants.md#v-torontomu) 

- :material-rename: **Run ID:** V-TorontoMU-DFReeKLIM-v2 
- :fontawesome-solid-user-group: **Participant:** V-TorontoMU 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/1/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `b6a6b6afe2940b4e01b7f6dacd0800a1` 
- :material-text: **Run description:** The code leverages the pyTerrier library to perform information retrieval using the DFReeKLIM weighting model, indexing preprocessed documents from specific crisis datasets. The queries, derived from user profiles, are then processed and matched against the indexed corpus to retrieve and rank relevant documents based on the significance of terms in the document collection relative to their occurrence in the query. The code doesn't directly calculate "importance" in a traditional sense. Instead, it uses the DFReeKLIM weighting model within the pyTerrier library to rank documents for a given query, which internally evaluates the significance of terms in the document collection relative to their occurrence in the query, thereby providing an implicit measure of document importance for a specific query. 

---
#### V-TorontoMU_SBERT_Semanti 
[**`Participants`**](./participants.md#v-torontomu) 

- :material-rename: **Run ID:** V-TorontoMU_SBERT_Semanti 
- :fontawesome-solid-user-group: **Participant:** V-TorontoMU 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/1/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `72815f44bff004abd897edd261465bbb` 
- :material-text: **Run description:** Leveraging the 'paraphrase-distilroberta-base-v1' Sentence-BERT model, the run achieves high-dimensional semantic embeddings of the queries and facts, capturing intricate linguistic nuances. Through cosine similarity metrics, the result discerns the semantic proximity between these embeddings, culminating in a refined selection of the top 200 contextually-aligned texts for each query. The code evaluates textual importance via the computation of cosine similarity between high-dimensional semantic embeddings derived from the Sentence-BERT model, where augmented similarity scores signify enhanced contextual congruence with the query. 

---
#### V-TorontoMU_USE_4 
[**`Participants`**](./participants.md#v-torontomu) 

- :material-rename: **Run ID:** V-TorontoMU_USE_4 
- :fontawesome-solid-user-group: **Participant:** V-TorontoMU 
- :material-format-text: **Track:** CrisisFACTs 
- :material-calendar: **Year:** 2023 
- :material-upload: **Submission:** 9/2/2023 
- :fontawesome-solid-user-gear: **Type:** automatic 
- :material-fingerprint: **MD5:** `dc9ce762d20a12359f4469df39850929` 
- :material-text: **Run description:** The code leverages the Universal Sentence Encoder (USE) to transform both textual documents and user queries into dense vector embeddings. Utilizing cosine similarity, it assesses and ranks the semantic proximity between these embeddings, thereby identifying and prioritizing documents that are most relevant to user inquiries. The code employs the Universal Sentence Encoder (USE) to generate embeddings for texts and user queries, facilitating semantic comparisons. It then ranks each text by its highest similarity score relative to the queries, ensuring documents most pertinent to user inquiries are highlighted. 

---
